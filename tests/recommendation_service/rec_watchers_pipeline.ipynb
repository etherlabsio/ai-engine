{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform approximate search against reference features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Recommendation project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, '/Users/shashank/Workspace/Orgs/Ether/ai-engine/services/recommendation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json as js\n",
    "from scipy.spatial.distance import cosine\n",
    "import pickle\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from collections import Counter, OrderedDict\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorize import Vectorizer\n",
    "from watchers import RecWatchers\n",
    "from explain import Explainability\n",
    "from utils import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_json(data, filename):\n",
    "    with open(filename + \".json\", \"w\", encoding=\"utf-8\") as f_:\n",
    "        js.dump(data, f_, ensure_ascii=False, indent=4)\n",
    "\n",
    "def read_json(json_file):\n",
    "    with open(json_file) as f_:\n",
    "        meeting = js.load(f_)\n",
    "    return meeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = [\"NN\", \"NNS\", \"NNP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate Sentence Encoder Lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto3 import client, session\n",
    "from botocore.client import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_config = Config(\n",
    "        connect_timeout=180,\n",
    "        read_timeout=300,\n",
    "        retries={\"max_attempts\": 2},\n",
    "        region_name=\"us-east-1\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = client(\"lambda\", config=aws_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = Vectorizer(lambda_client, \"sentence-encoder-lambda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_sess = session.Session(aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "conn = s3_sess.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = \"io.etherlabs.staging2.contexts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load reference features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"features/reference_user_text_vector.pickle\", 'rb') as f_:\n",
    "    reference_user_vector_data = pickle.load(f_)\n",
    "\n",
    "with open(\"features/reference_user_kw_vector.pickle\", 'rb') as f_:\n",
    "    reference_user_kw_vector_data = pickle.load(f_)\n",
    "    \n",
    "ref_user_dict = read_json(\"data/reference_prod_user.json\")\n",
    "ref_user_info_dict = {k: ref_user_dict[k][\"keywords\"] for k in ref_user_dict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.86832980505137"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9000**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.1357092861044"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "num_buckets=200\n",
    "hash_size=16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_url = \"https://hooks.slack.com/services/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\"\n",
    "prod_url = \"https://hooks.slack.com/services/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\"\n",
    "watcher_obj = RecWatchers(\n",
    "            reference_user_dict=ref_user_dict,\n",
    "            user_vector_data=reference_user_kw_vector_data,\n",
    "            vectorizer=vec,\n",
    "            s3_client=conn,\n",
    "            web_hook_url=staging_url,\n",
    "            num_buckets=num_buckets,\n",
    "            hash_size=hash_size\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0bbbfe84-c661-45af-8d0f-fcd5258bba38': 0,\n",
       " '1a215425-8449-4fca-ba95-7d768b595b80': 0,\n",
       " '84fbaa66-a247-4ea2-9ae0-53f3a2e519d6': 0,\n",
       " 'c66797a9-2e6d-46ad-9573-926e57f7dac3': 0,\n",
       " '2c944512-17a0-4912-9a16-6a3408da807c': 0,\n",
       " '7e7ccbba-232d-411a-a95a-d3f244a35f40': 0,\n",
       " '75bdf310-110b-4b8f-ab88-b16fafce920e': 0,\n",
       " 'b4a57b25-de68-446c-ac99-0f856d3fe4d5': 0,\n",
       " 'b1e8787a-9a1f-4859-ac11-cbb6a8124fd9': 0,\n",
       " '65bb8395-2fb5-4409-a4bb-59bb707f1375': 0,\n",
       " 'fb52cb66-3aec-4795-aee3-8ccfd904d315': 0,\n",
       " '62b6ae1d-7f83-4b0b-b205-5f7c72bc3368': 0,\n",
       " 'ecfeeb75-7f0a-4d47-af1e-bd513929264a': 0,\n",
       " '8d6db5f7-d9b7-4c54-ba38-fe710ffcaf3f': 0,\n",
       " '81a3e154-6937-4fce-ba1c-f972faa209b2': 0}"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watcher_obj.us.num_features_in_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0bbbfe84-c661-45af-8d0f-fcd5258bba38': 355,\n",
       " '1a215425-8449-4fca-ba95-7d768b595b80': 608,\n",
       " '84fbaa66-a247-4ea2-9ae0-53f3a2e519d6': 336,\n",
       " 'c66797a9-2e6d-46ad-9573-926e57f7dac3': 250,\n",
       " '2c944512-17a0-4912-9a16-6a3408da807c': 0,\n",
       " '7e7ccbba-232d-411a-a95a-d3f244a35f40': 1016,\n",
       " '75bdf310-110b-4b8f-ab88-b16fafce920e': 247,\n",
       " 'b4a57b25-de68-446c-ac99-0f856d3fe4d5': 361,\n",
       " 'b1e8787a-9a1f-4859-ac11-cbb6a8124fd9': 2164,\n",
       " '65bb8395-2fb5-4409-a4bb-59bb707f1375': 238,\n",
       " 'fb52cb66-3aec-4795-aee3-8ccfd904d315': 777,\n",
       " '62b6ae1d-7f83-4b0b-b205-5f7c72bc3368': 1194,\n",
       " 'ecfeeb75-7f0a-4d47-af1e-bd513929264a': 251,\n",
       " '8d6db5f7-d9b7-4c54-ba38-fe710ffcaf3f': 313,\n",
       " '81a3e154-6937-4fce-ba1c-f972faa209b2': 490}"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watcher_obj.featurize_reference_users()\n",
    "watcher_obj.us.num_features_in_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8600"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(list(watcher_obj.us.num_features_in_input.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_obj = [{\n",
    "\t\t\"id\": \"0f1aa05f2e944142a18974369bb9b789\",\n",
    "\t\t\"originalText\": \"We go into that issue because we read the from the Json field column tables. We we have their columns available, but we are not json and value from that column which is creating a problem but then you know, bring a that solution we should start needed it from that columns and only few places we've done ready need to do that the is the case. I think this not of code is the if you want to use the column portion of the values json log then there will be George changes setting. U a D then we start fields and all not so in case of meeting meeting and recording and markers. We have both Json fields and the call proper call call let let can that like that what I telling is will not do anything with that in the Json and value of d there right. We will replace the swing field with the Hyphen iPhone that's. The we are reading the ID from the Json. I'm not from the column. So in the days very while waiting from a database we need to change oriented B a force layer. I I will from the value where it is like it a relationship right but then the column or as become the that valuation can value. So if we start reading from actual columns we should not ask that's what I thinking less less. Let's see. And also just ping up existing links which we have doing the recording link right. That we it work automatically because those will not having open do id. Yeah, maybe if we can translate like internally and finish through that again. From layer where we to transfer it okay let me do five okay. I'll make the new are you working today. Yeah. Actually yesterday was what little bit from this cloud distribution task like basically what you what we to do is that me or we want to point it in and if the API part has a slack in okay part will do it routed to API p gateway but if it doesn't have any other if you are anything with before we have go to load we will go back to the same whatever we have the proxy right? So so the distribution and all to so that that part from started working yesterday I talk to trishant. So there are no blockage from there are point of view so we can like today be able to read that reference script for it. So after this I want to work on a P a service solution connect and is moving back to external load. The API will directed of external. So that with the proper point id.\",\n",
    "\t\t\"confidence\": 0.86578494,\n",
    "\t\t\"startTime\": \"2019-01-29T06:03:02Z\",\n",
    "\t\t\"endTime\": \"2019-01-29T06:03:14Z\",\n",
    "\t\t\"duration\": 12,\n",
    "\t\t\"recordingId\": \"9789538caa6d4857a7d6bf47584aafd8\",\n",
    "\t\t\"spokenBy\": \"0bbbfe84-c661-45af-8d0f-fcd5258bba38\",\n",
    "\t\t\"languageCode\": \"en-US\",\n",
    "\t\t\"transcriber\": \"google_speech_api\",\n",
    "\t\t\"status\": \"completed\",\n",
    "\t\t\"transcriptId\": \"e62bba58-b959-4aab-af7e-a75737fd736c\",\n",
    "\t\t\"createdAt\": \"2019-01-29T06:03:18.049750006Z\",\n",
    "\t\t\"updatedAt\": \"2019-01-29T06:03:39.783385299Z\"\n",
    "\t}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "segment_user_id = [str(uuid.UUID(u[\"spokenBy\"])) for u in segment_obj]\n",
    "segment_user_names = [ref_user_dict[u].get(\"name\") for u in segment_user_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Parshwa Nemi Jain']"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_user_id\n",
    "segment_user_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide query keywords or text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "slack_kw_input = \"response to John Searle, symbols with certain inputs, symbolic inputs, question of whether computers, type of computer, Artificial intelligence, famous philosophical arguments, real time John Searles Chinese room, fields of the philosophy, computer would be intelligent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"MapReduce is just a computing framework. HBase has nothing to do with it. That said, you can efficiently put or fetch data to/from HBase by writing MapReduce jobs. Alternatively you can write sequential programs using other HBase APIs, such as Java, to put or fetch the data. But we use Hadoop, HBase etc to deal with gigantic amounts of data, so that doesn't make much sense. Using normal sequential programs would be highly inefficient when your data is too huge. Coming back to the first part of your question, Hadoop is basically 2 things: a Distributed FileSystem (HDFS) + a Computation or Processing framework (MapReduce). Like all other FS, HDFS also provides us storage, but in a fault tolerant manner with high throughput and lower risk of data loss (because of the replication). But, being a FS, HDFS lacks random read and write access. This is where HBase comes into picture. It's a distributed, scalable, big data store, modelled after Google's BigTable. It stores data as key/value pairs. Hadoop is basically 3 things, a FS (Hadoop Distributed File System), a computation framework (MapReduce) and a management bridge (Yet Another Resource Negotiator). HDFS allows you store huge amounts of data in a distributed (provides faster read/write access) and redundant (provides better availability) manner. And MapReduce allows you to process this huge data in a distributed and parallel manner. But MapReduce is not limited to just HDFS. Being a FS, HDFS lacks the random read/write capability. It is good for sequential data access. And this is where HBase comes into picture. It is a NoSQL database that runs on top your Hadoop cluster and provides you random real-time read/write access to your data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "query_keywords = [\n",
    "    \"kind of update\", \n",
    "    \"summary segments\", \n",
    "    \"Brute Force fabric\", \n",
    "    \"share and dislike button\", \n",
    "    \"open the me tap\", \"invite Zoom\", \n",
    "    \"resume during playback\", \n",
    "    \"email and the\", \"select slack Channel\", \"share like the screen design\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_keywords = [w for w in slack_kw_input.split(\", \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['response to John Searle',\n",
       " 'symbols with certain inputs',\n",
       " 'symbolic inputs',\n",
       " 'question of whether computers',\n",
       " 'type of computer',\n",
       " 'Artificial intelligence',\n",
       " 'famous philosophical arguments',\n",
       " 'real time John Searles Chinese room',\n",
       " 'fields of the philosophy',\n",
       " 'computer would be intelligent']"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num results 5019\n"
     ]
    }
   ],
   "source": [
    "# watcher_obj.featurize_reference_users()\n",
    "hash_result = watcher_obj.perform_hash_query(input_list=query_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_user_dict, top_words, suggested_users = watcher_obj.get_recommended_watchers(input_query_list=query_keywords, \n",
    "                                                                                 input_kw_query=query_keywords,\n",
    "                                                                                hash_result=hash_result,\n",
    "                                                                                 segment_obj=segment_obj,\n",
    "                                                                                n_kw=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_hash_result = {ref_user_dict[u][\"name\"]: score for u, score in hash_result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Shashank', 0.656496062992126),\n",
       "             ('Karthik Muralidharan', 0.6139028475711893),\n",
       "             ('Trishanth Diwate', 0.611336032388664),\n",
       "             ('Arjun Kini', 0.6040816326530613),\n",
       "             ('Krishna Sai', 0.6038338658146964),\n",
       "             ('mithun', 0.5982142857142857),\n",
       "             ('Venkata Dikshit', 0.5928835489833642),\n",
       "             ('Deep Moradia', 0.5761772853185596),\n",
       "             ('Reagan Rewop', 0.5585585585585585),\n",
       "             ('Nisha Yadav', 0.552),\n",
       "             ('Vani', 0.5252100840336135),\n",
       "             ('Vamshi Krishna', 0.5098684210526315),\n",
       "             ('Parshwa Nemi Jain', 0.4732394366197183),\n",
       "             ('Shubham', 0.46215139442231074)])"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watcher_obj.utils.sort_dict_by_value(named_hash_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Shashank': 0.9502228120635137,\n",
       " 'Karthik Muralidharan': 0.7296724191256977,\n",
       " 'Arjun Kini': 0.6390368491362369,\n",
       " 'Trishanth Diwate': 0.6215599091513562}"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_user_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['small ontology',\n",
       " 'algorithmic side',\n",
       " 'Binary',\n",
       " 'language model',\n",
       " 'create life functions',\n",
       " 'data simulation',\n",
       " 'fundamental concepts',\n",
       " 'object objective',\n",
       " 'building Lambda functions',\n",
       " 'examine subject implementation']"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shashank', 'Karthik Muralidharan']"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggested_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post to Slack for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import jsonlines\n",
    "import logging\n",
    "import json as js\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "import requests\n",
    "import numpy as np\n",
    "import uuid\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_to_slack(\n",
    "    instance_id, segment_keyphrase_list, user_list, user_scores, suggested_user_list,segment_user_names, word_list\n",
    "):\n",
    "    input_keyphrase_list = segment_keyphrase_list\n",
    "\n",
    "    service_name = \"recommendation-service\"\n",
    "    msg_text = \"*Recommended users for meeting: {}* \\n *Segment summary*: ```{}```\\n\".format(\n",
    "        instance_id, _reformat_list_to_text(input_keyphrase_list)\n",
    "    )\n",
    "\n",
    "    if len(user_list) == 0:\n",
    "        msg_format = \"[{}]: {} *NA*: `No recommendations available for this segment...`\".format(\n",
    "                service_name,\n",
    "                msg_text)\n",
    "\n",
    "    else:\n",
    "        msg_format = \"[{}]: {} *Segment Users*: ```{}```\\n*Related Users*: ```{}```\\n *User Confidence Scores*: ```{}```\\n *Suggested Users*: ```{}```\\n *Related Words*: ```{}```\".format(\n",
    "            service_name,\n",
    "            msg_text,\n",
    "            _reformat_list_to_text(segment_user_names),\n",
    "            _reformat_list_to_text(user_list),\n",
    "            _reformat_list_to_text(user_scores),\n",
    "            _reformat_list_to_text(suggested_user_list),\n",
    "            _reformat_list_to_text(word_list),\n",
    "        )\n",
    "\n",
    "    slack_payload = {\"text\": msg_format}\n",
    "    requests.post(\n",
    "        url=staging_url, data=js.dumps(slack_payload).encode()\n",
    "    )\n",
    "\n",
    "def _reformat_list_to_text(input_list):\n",
    "    try:\n",
    "        if type(input_list[0]) != str:\n",
    "            formatted_text = \", \".join(\n",
    "                [\"{:.2f}\".format(i) for i in input_list]\n",
    "            )\n",
    "        else:\n",
    "            formatted_text = \", \".join([str(w) for w in input_list])\n",
    "    except Exception as e:\n",
    "        formatted_text = input_list\n",
    "        logger.warning(e)\n",
    "\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_validation_data(\n",
    "    input_query,\n",
    "    user_list,\n",
    "    user_scores,\n",
    "    suggested_user_list,\n",
    "    word_list,\n",
    "    segment_users,\n",
    "    instance_id=None,\n",
    "    context_id=None,\n",
    "    segment_obj=None,\n",
    "    upload=False\n",
    "):\n",
    "    if instance_id is None:\n",
    "        instance_id = \"\"\n",
    "        context_id = \"\"\n",
    "        segment_id = \"\"\n",
    "    \n",
    "    validation_dict = {}\n",
    "    for i in range(len(input_query)):\n",
    "        validation_dict.update(\n",
    "            {\n",
    "                \"text\": input_query,\n",
    "                \"labels\": user_list,\n",
    "                \"meta\": {\n",
    "                    \"instanceId\": instance_id,\n",
    "                    \"segmentId\": segment_id,\n",
    "                    \"suggestedUsers\": suggested_user_list,\n",
    "                    \"userScore\": user_scores,\n",
    "                    \"keyphrases\": input_query,\n",
    "                    \"relatedWords\": word_list,\n",
    "                    \"positiveLabels\": segment_users,\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    write_to_jsonl(validation_dict)\n",
    "\n",
    "    if upload:\n",
    "        upload_validation_data(\n",
    "            validation_dict=validation_dict, instance_id=instance_id, context_id=context_id, delete=False\n",
    "        )\n",
    "\n",
    "def write_to_jsonl(validation_dict, prefix=\"watchers_\", file_name=None):\n",
    "    validation_id = hash_sha_object()\n",
    "    save_dir = \"/Users/shashank/Workspace/Orgs/Ether/ai-engine/tests/recommendation_service/validation/\"\n",
    "    file_name = prefix + validation_id + \".jsonl\"\n",
    "    with jsonlines.open(os.path.join(save_dir, file_name), mode=\"w\") as writer:\n",
    "        writer.write(validation_dict)\n",
    "\n",
    "def upload_validation_data(\n",
    "    validation_dict, instance_id, context_id, prefix=\"watchers_\", delete=False\n",
    "):\n",
    "    validation_id = hash_sha_object()\n",
    "    file_name = prefix + instance_id + \"_\" + validation_id + \".jsonl\"\n",
    "    with jsonlines.open(file_name, mode=\"w\") as writer:\n",
    "        writer.write(validation_dict)\n",
    "\n",
    "    s3_path = \"validation/recommendations/\" + file_name\n",
    "\n",
    "    try:\n",
    "        s3_client.upload_to_s3(\n",
    "            file_name=file_name, object_name=s3_path\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    if delete:\n",
    "        # Once uploading is successful, check if NPZ exists on disk and delete it\n",
    "        local_path = Path(file_name).absolute()\n",
    "        if os.path.exists(local_path):\n",
    "            os.remove(local_path)\n",
    "        \n",
    "def hash_sha_object() -> str:\n",
    "    uid = uuid.uuid4()\n",
    "    uid = str(uid)\n",
    "    hash_object = hashlib.sha1(uid.encode())\n",
    "    hash_str = hash_object.hexdigest()\n",
    "    return hash_str\n",
    "\n",
    "def normalize(score, scores_list):\n",
    "    normalized_score = (score - np.mean(scores_list)) / (\n",
    "        np.max(scores_list) - np.min(scores_list)\n",
    "    )\n",
    "    return normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_to_slack(\n",
    "    instance_id=\"s2-test\",\n",
    "    segment_keyphrase_list=query_keywords,\n",
    "    user_list=list(top_user_dict.keys()),\n",
    "    user_scores=list(top_user_dict.values()),\n",
    "    suggested_user_list= suggested_users,\n",
    "    segment_user_names = segment_user_names,\n",
    "    word_list=top_words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_user_names = \"\"\n",
    "make_validation_data(\n",
    "    input_query=query_keywords,\n",
    "    user_list=list(top_user_dict.keys()),\n",
    "    user_scores=list(top_user_dict.values()),\n",
    "    suggested_user_list=suggested_users,\n",
    "    word_list=top_words,\n",
    "    segment_users=segment_user_names,\n",
    "    upload=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform approximate search against reference features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Recommendation project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, '/Users/shashank/Workspace/Orgs/Ether/ai-engine/services/recommendation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json as js\n",
    "from scipy.spatial.distance import cosine\n",
    "import pickle\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from collections import Counter, OrderedDict\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorize import Vectorizer\n",
    "from watchers import RecWatchers\n",
    "from explain import Explainability\n",
    "from utils import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_json(data, filename):\n",
    "    with open(filename + \".json\", \"w\", encoding=\"utf-8\") as f_:\n",
    "        js.dump(data, f_, ensure_ascii=False, indent=4)\n",
    "\n",
    "def read_json(json_file):\n",
    "    with open(json_file) as f_:\n",
    "        meeting = js.load(f_)\n",
    "    return meeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = [\"NN\", \"NNS\", \"NNP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate Sentence Encoder Lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto3 import client, session\n",
    "from botocore.client import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_config = Config(\n",
    "        connect_timeout=180,\n",
    "        read_timeout=300,\n",
    "        retries={\"max_attempts\": 2},\n",
    "        region_name=\"us-east-1\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = client(\"lambda\", config=aws_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = Vectorizer(lambda_client, \"sentence-encoder-lambda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_sess = session.Session(aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "conn = s3_sess.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = \"io.etherlabs.staging2.contexts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load reference features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reference_user_text_vector.pickle\", 'rb') as f_:\n",
    "    reference_user_vector_data = pickle.load(f_)\n",
    "\n",
    "with open(\"reference_user_kw_vector.pickle\", 'rb') as f_:\n",
    "    reference_user_kw_vector_data = pickle.load(f_)\n",
    "    \n",
    "ref_user_dict = read_json(\"reference_prod_user.json\")\n",
    "ref_user_info_dict = {k: ref_user_dict[k][\"keywords\"] for k in ref_user_dict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.86832980505137"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9000**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.1357092861044"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "num_buckets=200\n",
    "hash_size=16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_url = \"https://hooks.slack.com/services/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\"\n",
    "prod_url = \"https://hooks.slack.com/services/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\"\n",
    "watcher_obj = RecWatchers(\n",
    "            reference_user_dict=ref_user_dict,\n",
    "            user_vector_data=reference_user_kw_vector_data,\n",
    "            vectorizer=vec,\n",
    "            s3_client=conn,\n",
    "            web_hook_url=staging_url,\n",
    "            num_buckets=num_buckets,\n",
    "            hash_size=hash_size\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0bbbfe84-c661-45af-8d0f-fcd5258bba38': 0,\n",
       " '1a215425-8449-4fca-ba95-7d768b595b80': 0,\n",
       " '84fbaa66-a247-4ea2-9ae0-53f3a2e519d6': 0,\n",
       " 'c66797a9-2e6d-46ad-9573-926e57f7dac3': 0,\n",
       " '2c944512-17a0-4912-9a16-6a3408da807c': 0,\n",
       " '7e7ccbba-232d-411a-a95a-d3f244a35f40': 0,\n",
       " '75bdf310-110b-4b8f-ab88-b16fafce920e': 0,\n",
       " 'b4a57b25-de68-446c-ac99-0f856d3fe4d5': 0,\n",
       " 'b1e8787a-9a1f-4859-ac11-cbb6a8124fd9': 0,\n",
       " '65bb8395-2fb5-4409-a4bb-59bb707f1375': 0,\n",
       " 'fb52cb66-3aec-4795-aee3-8ccfd904d315': 0,\n",
       " '62b6ae1d-7f83-4b0b-b205-5f7c72bc3368': 0,\n",
       " 'ecfeeb75-7f0a-4d47-af1e-bd513929264a': 0,\n",
       " '8d6db5f7-d9b7-4c54-ba38-fe710ffcaf3f': 0,\n",
       " '81a3e154-6937-4fce-ba1c-f972faa209b2': 0}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watcher_obj.us.num_features_in_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0bbbfe84-c661-45af-8d0f-fcd5258bba38': 355,\n",
       " '1a215425-8449-4fca-ba95-7d768b595b80': 608,\n",
       " '84fbaa66-a247-4ea2-9ae0-53f3a2e519d6': 336,\n",
       " 'c66797a9-2e6d-46ad-9573-926e57f7dac3': 250,\n",
       " '2c944512-17a0-4912-9a16-6a3408da807c': 0,\n",
       " '7e7ccbba-232d-411a-a95a-d3f244a35f40': 1016,\n",
       " '75bdf310-110b-4b8f-ab88-b16fafce920e': 247,\n",
       " 'b4a57b25-de68-446c-ac99-0f856d3fe4d5': 361,\n",
       " 'b1e8787a-9a1f-4859-ac11-cbb6a8124fd9': 2164,\n",
       " '65bb8395-2fb5-4409-a4bb-59bb707f1375': 238,\n",
       " 'fb52cb66-3aec-4795-aee3-8ccfd904d315': 777,\n",
       " '62b6ae1d-7f83-4b0b-b205-5f7c72bc3368': 1194,\n",
       " 'ecfeeb75-7f0a-4d47-af1e-bd513929264a': 251,\n",
       " '8d6db5f7-d9b7-4c54-ba38-fe710ffcaf3f': 313,\n",
       " '81a3e154-6937-4fce-ba1c-f972faa209b2': 490}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watcher_obj.featurize_reference_users()\n",
    "watcher_obj.us.num_features_in_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8600"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(list(watcher_obj.us.num_features_in_input.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide query keywords or text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "slack_kw_input = \"domain this whole segment, domain topic cluster, cluster which has AC, demand topic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"MapReduce is just a computing framework. HBase has nothing to do with it. That said, you can efficiently put or fetch data to/from HBase by writing MapReduce jobs. Alternatively you can write sequential programs using other HBase APIs, such as Java, to put or fetch the data. But we use Hadoop, HBase etc to deal with gigantic amounts of data, so that doesn't make much sense. Using normal sequential programs would be highly inefficient when your data is too huge. Coming back to the first part of your question, Hadoop is basically 2 things: a Distributed FileSystem (HDFS) + a Computation or Processing framework (MapReduce). Like all other FS, HDFS also provides us storage, but in a fault tolerant manner with high throughput and lower risk of data loss (because of the replication). But, being a FS, HDFS lacks random read and write access. This is where HBase comes into picture. It's a distributed, scalable, big data store, modelled after Google's BigTable. It stores data as key/value pairs. Hadoop is basically 3 things, a FS (Hadoop Distributed File System), a computation framework (MapReduce) and a management bridge (Yet Another Resource Negotiator). HDFS allows you store huge amounts of data in a distributed (provides faster read/write access) and redundant (provides better availability) manner. And MapReduce allows you to process this huge data in a distributed and parallel manner. But MapReduce is not limited to just HDFS. Being a FS, HDFS lacks the random read/write capability. It is good for sequential data access. And this is where HBase comes into picture. It is a NoSQL database that runs on top your Hadoop cluster and provides you random real-time read/write access to your data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "query_keywords = [\n",
    "    \"kind of update\", \n",
    "    \"summary segments\", \n",
    "    \"Brute Force fabric\", \n",
    "    \"share and dislike button\", \n",
    "    \"open the me tap\", \"invite Zoom\", \n",
    "    \"resume during playback\", \n",
    "    \"email and the\", \"select slack Channel\", \"share like the screen design\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_keywords = [w for w in slack_kw_input.split(\", \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['domain this whole segment',\n",
       " 'domain topic cluster',\n",
       " 'cluster which has AC',\n",
       " 'demand topic']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num results 3466\n"
     ]
    }
   ],
   "source": [
    "# watcher_obj.featurize_reference_users()\n",
    "hash_result = watcher_obj.perform_hash_query(input_list=query_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_user_dict, top_words, suggested_users = watcher_obj.get_recommended_watchers(input_query_list=query_keywords, \n",
    "                                                                                 input_kw_query=query_keywords,\n",
    "                                                                                hash_result=hash_result,\n",
    "                                                                                n_kw=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_hash_result = {ref_user_dict[u][\"name\"]: score for u, score in hash_result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Reagan Rewop', 0.45302445302445304),\n",
       "             ('Arjun Kini', 0.44081632653061226),\n",
       "             ('Shashank', 0.4389763779527559),\n",
       "             ('Venkata Dikshit', 0.422365988909427),\n",
       "             ('Karthik Muralidharan', 0.4128978224455611),\n",
       "             ('Krishna Sai', 0.4057507987220447),\n",
       "             ('mithun', 0.4017857142857143),\n",
       "             ('Trishanth Diwate', 0.4008097165991903),\n",
       "             ('Parshwa Nemi Jain', 0.4),\n",
       "             ('Shubham', 0.3904382470119522),\n",
       "             ('Deep Moradia', 0.3268698060941828),\n",
       "             ('Vani', 0.31512605042016806),\n",
       "             ('Vamshi Krishna', 0.31085526315789475),\n",
       "             ('Nisha Yadav', 0.248)])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watcher_obj.utils.sort_dict_by_value(named_hash_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['domains groups',\n",
       " 'Aws Side',\n",
       " 'answer service current phase data',\n",
       " 'current schema',\n",
       " 'entity graph service']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reagan Rewop', 'Shashank']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggested_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post to Slack for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import jsonlines\n",
    "import logging\n",
    "import json as js\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "import requests\n",
    "import numpy as np\n",
    "import uuid\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_to_slack(\n",
    "    instance_id, segment_keyphrase_list, user_list, user_scores, suggested_user_list, word_list\n",
    "):\n",
    "    input_keyphrase_list = segment_keyphrase_list\n",
    "\n",
    "    service_name = \"recommendation-service\"\n",
    "    msg_text = \"*Recommended users for meeting: {}* \\n *Segment summary*: ```{}```\\n\".format(\n",
    "        instance_id, _reformat_list_to_text(input_keyphrase_list)\n",
    "    )\n",
    "\n",
    "    msg_format = \"[{}]: {} *Related Users*: ```{}```\\n *User Confidence Scores*: ```{}```\\n *Suggested Users*: ```{}```\\n *Related Words*: ```{}```\".format(\n",
    "        service_name,\n",
    "        msg_text,\n",
    "        _reformat_list_to_text(user_list),\n",
    "        _reformat_list_to_text(user_scores),\n",
    "        _reformat_list_to_text(suggested_user_list),\n",
    "        _reformat_list_to_text(word_list),\n",
    "    )\n",
    "\n",
    "    slack_payload = {\"text\": msg_format}\n",
    "    requests.post(\n",
    "        url=staging_url, data=js.dumps(slack_payload).encode()\n",
    "    )\n",
    "\n",
    "def _reformat_list_to_text(input_list):\n",
    "    try:\n",
    "        if type(input_list[0]) != str:\n",
    "            formatted_text = \", \".join(\n",
    "                [\"{:.2f}\".format(i) for i in input_list]\n",
    "            )\n",
    "        else:\n",
    "            formatted_text = \", \".join([str(w) for w in input_list])\n",
    "    except Exception as e:\n",
    "        formatted_text = input_list\n",
    "        logger.warning(e)\n",
    "\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_validation_data(\n",
    "    input_query,\n",
    "    user_list,\n",
    "    user_scores,\n",
    "    suggested_user_list,\n",
    "    word_list,\n",
    "    segment_users,\n",
    "    instance_id=None,\n",
    "    context_id=None,\n",
    "    segment_obj=None,\n",
    "    upload=False\n",
    "):\n",
    "    if instance_id is None:\n",
    "        instance_id = \"\"\n",
    "        context_id = \"\"\n",
    "        segment_id = \"\"\n",
    "    \n",
    "    validation_dict = {}\n",
    "    for i in range(len(input_query)):\n",
    "        validation_dict.update(\n",
    "            {\n",
    "                \"text\": input_query,\n",
    "                \"labels\": user_list,\n",
    "                \"meta\": {\n",
    "                    \"instanceId\": instance_id,\n",
    "                    \"segmentId\": segment_id,\n",
    "                    \"suggestedUsers\": suggested_user_list,\n",
    "                    \"userScore\": user_scores,\n",
    "                    \"keyphrases\": input_query,\n",
    "                    \"relatedWords\": word_list,\n",
    "                    \"positiveLabels\": segment_users,\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    write_to_jsonl(validation_dict)\n",
    "\n",
    "    if upload:\n",
    "        upload_validation_data(\n",
    "            validation_dict=validation_dict, instance_id=instance_id, context_id=context_id, delete=False\n",
    "        )\n",
    "\n",
    "def write_to_jsonl(validation_dict, prefix=\"watchers_\", file_name=None):\n",
    "    validation_id = hash_sha_object()\n",
    "    save_dir = \"/Users/shashank/Workspace/Orgs/Ether/ai-engine/tests/recommendation_service/validation/\"\n",
    "    file_name = prefix + validation_id + \".jsonl\"\n",
    "    with jsonlines.open(os.path.join(save_dir, file_name), mode=\"w\") as writer:\n",
    "        writer.write(validation_dict)\n",
    "\n",
    "def upload_validation_data(\n",
    "    validation_dict, instance_id, context_id, prefix=\"watchers_\", delete=False\n",
    "):\n",
    "    validation_id = hash_sha_object()\n",
    "    file_name = prefix + instance_id + \"_\" + validation_id + \".jsonl\"\n",
    "    with jsonlines.open(file_name, mode=\"w\") as writer:\n",
    "        writer.write(validation_dict)\n",
    "\n",
    "    s3_path = \"validation/recommendations/\" + file_name\n",
    "\n",
    "    try:\n",
    "        s3_client.upload_to_s3(\n",
    "            file_name=file_name, object_name=s3_path\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    if delete:\n",
    "        # Once uploading is successful, check if NPZ exists on disk and delete it\n",
    "        local_path = Path(file_name).absolute()\n",
    "        if os.path.exists(local_path):\n",
    "            os.remove(local_path)\n",
    "        \n",
    "def hash_sha_object() -> str:\n",
    "    uid = uuid.uuid4()\n",
    "    uid = str(uid)\n",
    "    hash_object = hashlib.sha1(uid.encode())\n",
    "    hash_str = hash_object.hexdigest()\n",
    "    return hash_str\n",
    "\n",
    "def normalize(score, scores_list):\n",
    "    normalized_score = (score - np.mean(scores_list)) / (\n",
    "        np.max(scores_list) - np.min(scores_list)\n",
    "    )\n",
    "    return normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_to_slack(\n",
    "    instance_id=\"s2-test\",\n",
    "    segment_keyphrase_list=query_keywords,\n",
    "    user_list=list(top_user_dict.keys()),\n",
    "    user_scores=list(top_user_dict.values()),\n",
    "    suggested_user_list= suggested_users,\n",
    "    word_list=top_words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_user_names = \"\"\n",
    "make_validation_data(\n",
    "    input_query=query_keywords,\n",
    "    user_list=list(top_user_dict.keys()),\n",
    "    user_scores=list(top_user_dict.values()),\n",
    "    suggested_user_list=suggested_users,\n",
    "    word_list=top_words,\n",
    "    segment_users=segment_user_names,\n",
    "    upload=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T12:57:44.812204Z",
     "start_time": "2019-07-01T12:57:42.428900Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"/home/ray__/CS/org/etherlabs/community_detection/\")\n",
    "import json as js\n",
    "import text_preprocessing.preprocess as tp\n",
    "import torch\n",
    "import logging\n",
    "import networkx as nx\n",
    "import math\n",
    "from scipy.spatial.distance import cosine\n",
    "import community\n",
    "from datetime import datetime\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertModel\n",
    "from pytorch_pretrained_bert.modeling import BertPreTrainedModel, BertPreTrainingHeads\n",
    "import numpy as np\n",
    "import iso8601\n",
    "bert_config = {}\n",
    "bert_config[\"tokenizer\"] = 'bert-base-uncased'\n",
    "bert_config[\"config\"] = '/home/ray__/CS/org/etherlabs/ai-engine_pants/ai-engine/mind_files/bert_config.json'\n",
    "bert_config[\"bert_model\"] = 'bert-base-uncased'\n",
    "bert_config[\"load_file\"] = \"/home/ray__/CS/org/etherlabs/ai-engine_pants/ai-engine/mind_files/\"\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "class BertForPreTraining_custom(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForPreTraining_custom, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None):\n",
    "        output_all_encoded_layers=True\n",
    "        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n",
    "                                                   output_all_encoded_layers=output_all_encoded_layers)\n",
    "        if output_all_encoded_layers:\n",
    "            sequence_output_pred = sequence_output[-1]\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output_pred, pooled_output)\n",
    "\n",
    "        return prediction_scores, seq_relationship_score, sequence_output, pooled_output \n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def getNSPScore(sample_text):\n",
    "    m = torch.nn.Softmax()\n",
    "\n",
    "    tokenized_text = tokenizer.tokenize(sample_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = [0]*tokenized_text.index('[SEP]')+[1]*(len(tokenized_text)-tokenized_text.index('[SEP]'))\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    pred_score, seq_rel, seq_out, pool_out = model1(tokens_tensor, segments_tensors)\n",
    "    return m(seq_rel).detach().numpy()[0][0] #returns probability of being next sentence\n",
    "\n",
    "def getSentMatchScore(sent1, sent2, nsp_dampening_factor = 0.7):\n",
    "    sent1_feats = getBERTFeatures(model1, sent1, attn_head_idx)\n",
    "    sent2_feats = getBERTFeatures(model1, sent2, attn_head_idx)\n",
    "    cosine_distance = 1- cosine(sent1_feats, sent2_feats)\n",
    "    nsp_input1 = sent1+' [SEP] '+sent2\n",
    "    nsp_input2 = sent2+' [SEP] '+sent1\n",
    "    nsp_score_1 = getNSPScore(nsp_input1)\n",
    "    nsp_score_2 = getNSPScore(nsp_input2)\n",
    "    nsp_score = np.mean([nsp_score_1,nsp_score_2])*nsp_dampening_factor\n",
    "    len_diff = abs(len(sent1.split(' '))-len(sent2.split(' ')))\n",
    "    if len_diff>2*(min(len(sent1.split(' ')),len(sent2.split(' ')))):\n",
    "        #give more weight to nsp if the sentences of largely varying lengths\n",
    "        score = 0.4*cosine_distance+0.6*nsp_score\n",
    "    else:\n",
    "        score = np.mean([cosine_distance,nsp_score])\n",
    "    #print (\"nsp score -> \" + str(nsp_score))\n",
    "    #print (\"cosine score -> \" + str(cosine_distance))\n",
    "    return score\n",
    "\n",
    "def getSentMatchScore_wfeature(sent1, sent2, sent1_feats, sent2_feats, nsp_dampening_factor = 0.7):\n",
    "    cosine_distance = 1-cosine(sent1_feats, sent2_feats)\n",
    "    #return cosine_distance\n",
    "    nsp_input1 = sent1+' [SEP] '+sent2\n",
    "    #nsp_input2 = sent2+' [SEP] '+sent1\n",
    "    nsp_score_1 = getNSPScore(nsp_input1)\n",
    "    #nsp_score_2 = getNSPScore(nsp_input2)\n",
    "    nsp_score = nsp_score_1 * nsp_dampening_factor\n",
    "    #nsp_score = nsp_score_1*nsp_dampening_factor\n",
    "    len_diff = abs(len(sent1.split(' '))-len(sent2.split(' ')))\n",
    "    if len_diff>2*(min(len(sent1.split(' ')),len(sent2.split(' ')))):\n",
    "        #give more weight to nsp if the sentences of largely varying lengths\n",
    "        score = 0.4*cosine_distance+0.6*nsp_score\n",
    "    else:\n",
    "        score = np.mean([cosine_distance,nsp_score])\n",
    "    return score\n",
    "\n",
    "def getSentMatchScore_wfeature_cosine(sent1, sent2, sent1_feats, sent2_feats, nsp_dampening_factor = 0.7):\n",
    "    cosine_distance = 1-cosine(sent1_feats, sent2_feats)\n",
    "    return cosine_distance\n",
    "\n",
    "def getSentMatchScore_wfeature_test(sent1, sent2, sent1_feats, sent2_feats, nsp_dampening_factor = 0.7):\n",
    "    cosine_distance = 1-cosine(sent1_feats, sent2_feats)\n",
    "    nsp_input1 = sent1+' [SEP] '+sent2\n",
    "    nsp_input2 = sent2+' [SEP] '+sent1\n",
    "    nsp_score_1 = getNSPScore(nsp_input1)\n",
    "    nsp_score_2 = getNSPScore(nsp_input2)\n",
    "    nsp_score = np.mean([nsp_score_1,nsp_score_2])*nsp_dampening_factor\n",
    "    #nsp_score = nsp_score_1*nsp_dampening_factor\n",
    "    len_diff = abs(len(sent1.split(' '))-len(sent2.split(' ')))\n",
    "    if len_diff>2*(min(len(sent1.split(' ')),len(sent2.split(' ')))):\n",
    "        #give more weight to nsp if the sentences of largely varying lengths\n",
    "        score = 0.4*cosine_distance+0.6*nsp_score\n",
    "    else:\n",
    "        score = np.mean([cosine_distance,nsp_score])\n",
    "    return score, cosine_distance, nsp_score\n",
    "\n",
    "def getBERTFeatures(model, text, attn_head_idx = -1): #attn_head_idx - index o[]\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    if len(tokenized_text)>200:\n",
    "        tokenized_text = tokenized_text[0:200]\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    _, _, seq_out, pool_out = model(tokens_tensor)\n",
    "    seq_out = list(getPooledFeatures(seq_out[attn_head_idx]).T)\n",
    "    #pool_out = list(pool_out.detach().numpy().T)\n",
    "    return seq_out\n",
    "\n",
    "def getPooledFeatures(np_array):\n",
    "    np_array = np_array.reshape(np_array.shape[1],np_array.shape[2]).detach().numpy()\n",
    "    np_array_mp = np.mean(np_array, axis=0).reshape(1, -1)\n",
    "    return np_array_mp\n",
    "\n",
    "def replaceContractions(text):\n",
    "    #text = text.lower()\n",
    "    c_filt_text = ''\n",
    "    for word in text.split(' '):\n",
    "        if word in contractions:\n",
    "            c_filt_text = c_filt_text+' '+contractions[word]\n",
    "        else:\n",
    "            c_filt_text = c_filt_text+' '+word\n",
    "    return c_filt_text\n",
    "\n",
    "def cleanText(text):\n",
    "    text = text.replace('\\\\n','')\n",
    "    text = text.replace('\\\\','')\n",
    "    #text = text.replace('\\t', '')\n",
    "    #text = re.sub('\\[(.*?)\\]','',text) #removes [this one]\n",
    "    text = re.sub('(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?\\s',\n",
    "                ' __url__ ',text) #remove urls\n",
    "    #text = re.sub('\\'','',text)\n",
    "    #text = re.sub(r'\\d+', ' __number__ ', text) #replaces numbers\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.replace('\\t', '')\n",
    "    text = text.replace('\\n', '')\n",
    "    return text\n",
    "\n",
    "def formatTime(tz_time, datetime_object=False):\n",
    "    isoTime = iso8601.parse_date(tz_time)\n",
    "    ts = isoTime.timestamp()\n",
    "    ts = datetime.utcfromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n",
    "\n",
    "    if datetime_object:\n",
    "        ts = datetime.fromisoformat(ts)\n",
    "    return ts\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List\n",
    "import logging\n",
    "import text_preprocessing.preprocess as tp\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Request:\n",
    "    segments: list\n",
    "\n",
    "def decode_json_request(body) -> Request:\n",
    "    req = body\n",
    "\n",
    "#     if isinstance(body, str):\n",
    "#         req = json.load(body)\n",
    "\n",
    "    def decode_segments(seg):\n",
    "        segments_text = list(map(lambda x:tp.preprocess(x['originalText'], stop_words=False, remove_punct=False), seg['segments']))\n",
    "        segments_data = seg['segments']\n",
    "        for index, segment in  enumerate(segments_data):\n",
    "            segments_data[index]['originalText'] = segments_text[index]\n",
    "        return segments_data\n",
    "\n",
    "    segments = decode_segments(req)\n",
    "    return Request(segments)\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertModel\n",
    "from pytorch_pretrained_bert.modeling import BertPreTrainedModel, BertPreTrainingHeads\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def loadmodel(model_config, mind):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_config['tokenizer'])\n",
    "    config_file = BertConfig.from_json_file(model_config['config'])\n",
    "    #logger.debug(\"config file\", extra={\"contains\": model_config})\n",
    "    bert_model = model_config['bert_model']\n",
    "    load_file = model_config['load_file'] + mind\n",
    "# config_file = BertConfig.from_json_file('services/community/bert_config.json')\n",
    "    model1 = BertForPreTraining_custom(config_file)\n",
    "    state_dict_1 = torch.load(load_file, map_location='cpu')\n",
    "    model1.load_state_dict(state_dict_1)\n",
    "    return model1\n",
    "\n",
    "def selectmodel(MindId):\n",
    "    return \"mind-\"+str(MindId)+\".bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T12:57:44.870977Z",
     "start_time": "2019-07-01T12:57:44.846083Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "class community_detection():\n",
    "    segments_list = []\n",
    "    model1 = None\n",
    "    def __init__(self, Request, model1):\n",
    "        self.segments_list = Request.segments\n",
    "        self.model1 = model1\n",
    "    #def parse_meeting(self, segments):\n",
    "    #    segments_data = list(map(lambda x: tp.preprocess(x['originalText'], stop_words=False, remove_punct=False), segments['segments']))\n",
    "    #    self.segments_list = segments['segments']\n",
    "    #    for index,seg in enumerate(self.segments_list):\n",
    "    #        self.segments_list[index]['originalText'] = segments_data[index]\n",
    "\n",
    "    def compute_feature_vector(self):\n",
    "        #tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        #config = BertConfig.from_json_file('services/community/bert_config.json')\n",
    "        #bert_model = 'bert-base-uncased'\n",
    "        #model1 = bert.BertForPreTraining_custom(config)\n",
    "        #state_dict_1 = torch.load('services/community/bert_10epc_se_1e-6_sl40.bin')\n",
    "        #state_dict_1 = torch.load('services/community/bert_10epc_inc_se+etherdata_1e-6_sl40_bt64.bin')\n",
    "        #model1.load_state_dict(state_dict_1)\n",
    "        #model1.eval()\n",
    "        graph_list = {}\n",
    "        fv = {}\n",
    "        index = 0\n",
    "        for segment in self.segments_list:\n",
    "            for sent in segment['originalText']:\n",
    "                graph_list[index] = (sent, segment['startTime'], segment['spokenBy'], segment['id'])\n",
    "                fv[index] = getBERTFeatures(self.model1, sent, attn_head_idx=-1)\n",
    "                index+=1\n",
    "        return fv, graph_list\n",
    "\n",
    "    def construct_graph(self, fv, graph_list):\n",
    "        meeting_graph = nx.Graph()\n",
    "        yetto_prune = []\n",
    "        c_weight = 0\n",
    "        for indexa, nodea in enumerate(graph_list.values()):\n",
    "            for indexb, nodeb in enumerate(graph_list.values()):\n",
    "                if indexb>indexa:\n",
    "                    c_weight = cosine(fv[indexa], fv[indexb])\n",
    "                    meeting_graph.add_edge(indexa, indexb, weight=c_weight)\n",
    "                    yetto_prune.append((indexa, indexb, c_weight))\n",
    "        return meeting_graph, yetto_prune\n",
    "\n",
    "    def prune_edges(self, meeting_graph, graph_list, yetto_prune,v=0.01):\n",
    "        yetto_prune = sorted(yetto_prune, key=lambda kv : kv[2], reverse=True)\n",
    "        yetto_prune = yetto_prune[:math.ceil(len(yetto_prune)*v)+1]\n",
    "        #logger.info(\"pruning value\", extra={\"v is : \": v})\n",
    "        print ({\"v is : \": v})\n",
    "        meeting_graph_pruned = nx.Graph()\n",
    "        for indexa, indexb, c_score in yetto_prune:\n",
    "            meeting_graph_pruned.add_edge(indexa, indexb)\n",
    "        return meeting_graph_pruned\n",
    "\n",
    "    def compute_louvian_community(self, meeting_graph_pruned, community_set):\n",
    "        #community_set = community.best_partition(meeting_graph_pruned,randomize=False,random_state=9)\n",
    "        #modularity_score = community.modularity(community_set, meeting_graph_pruned)\n",
    "        #logger.info(\"Community results\", extra={\"modularity score\":modularity_score})\n",
    "        #print ({\"modularity score\":modularity_score})\n",
    "        community_set_sorted = sorted(community_set.items(), key = lambda kv: kv[1], reverse=False)\n",
    "\n",
    "        return community_set_sorted\n",
    "\n",
    "    def refine_community(self, community_set_sorted, graph_list):\n",
    "        clusters = []\n",
    "        temp = []\n",
    "        prev_com = 0\n",
    "        for word,cluster in community_set_sorted:\n",
    "            if prev_com==cluster:\n",
    "                temp.append(word)\n",
    "            else:\n",
    "                clusters.append(temp)\n",
    "                temp = []\n",
    "                prev_com = cluster\n",
    "                temp.append(word)\n",
    "        timerange = []\n",
    "        temp = []\n",
    "        for cluster in clusters:\n",
    "            temp= []\n",
    "            for sent in cluster:\n",
    "                #temp.append(graph_list[sent])\n",
    "                #logger.info(\"segment values\", extra={\"segment\":self.segments_list})\n",
    "                temp.append(graph_list[sent])\n",
    "            if len(temp)!=0:\n",
    "                temp = list(set(temp))\n",
    "                temp = sorted(temp,key=lambda kv: kv[1], reverse=False)\n",
    "                timerange.append(temp)\n",
    "\n",
    "        return timerange\n",
    "\n",
    "    def group_community_by_time(self, timerange):\n",
    "        timerange_detailed = []\n",
    "        temp = []\n",
    "        flag = False\n",
    "        pims = {}\n",
    "        index_pim = 0\n",
    "        index_segment = 0\n",
    "        for index,com in enumerate(timerange):\n",
    "            temp = []\n",
    "            flag = False\n",
    "            #print (\"-----community-----\", index)\n",
    "            for (index1,(sent1,time1,user1, id1)), (index2,(sent2,time2,user2, id2)) in zip(enumerate(com[0:]),enumerate(com[1:])):\n",
    "                if id1!=id2:\n",
    "                    if ((formatTime( time2, True)-formatTime(time1, True)).seconds<=240):\n",
    "                        if (not flag):\n",
    "                            pims[index_pim] = {'segment'+str(index_segment):[sent1,time1,user1, id1]}\n",
    "                            index_segment+=1\n",
    "                            temp.append((sent1,time1,user1, id1))\n",
    "                        pims[index_pim]['segment'+str(index_segment)] = [sent2,time2,user2, id2]\n",
    "                        index_segment+=1\n",
    "                        temp.append((sent2,time2,user2,id2))\n",
    "                        flag=True\n",
    "                    else:\n",
    "                        if flag==True:\n",
    "                            index_pim+=1\n",
    "                            index_segment=0\n",
    "                        flag=False\n",
    "            if flag==True:\n",
    "                index_pim+=1\n",
    "                index_segment=0\n",
    "            timerange_detailed.append(temp)\n",
    "        return pims\n",
    "\n",
    "    def wrap_community_by_time(self, pims):\n",
    "        yet_to_combine = []\n",
    "        need_to_remove = []\n",
    "        inverse_dangling_pims = []\n",
    "        for index1,i in enumerate(pims.keys()):\n",
    "            for index2,j in enumerate(pims.keys()):\n",
    "                if index1!=index2:\n",
    "                    if pims[i]['segment0'][1] >= pims[j]['segment0'][1] and pims[i]['segment0'][1] <= pims[j]['segment'+str(len(pims[j].values())-1)][1]:\n",
    "                        if (j,i) not in yet_to_combine and i not in need_to_remove and j not in need_to_remove:\n",
    "                            yet_to_combine.append((i,j))\n",
    "                            need_to_remove.append(i)\n",
    "        for i,j in yet_to_combine:\n",
    "            for k in pims[i]:\n",
    "                if pims[i][k] not in pims[j].values():\n",
    "                    pims[j]['segment'+str(len(pims[j].values())-1)] = pims[i][k]\n",
    "                    continue\n",
    "        for i in need_to_remove:\n",
    "            pims.pop(i)\n",
    "\n",
    "        for index, p in enumerate(pims.keys()):\n",
    "            for seg in pims[p].keys():\n",
    "                pims[p][seg][0] = [' '.join(text for text in segment['originalText']) for segment in self.segments_list if segment['id']==pims[p][seg][3]]\n",
    "                inverse_dangling_pims.append(pims[p][seg][3])\n",
    "\n",
    "        c_len = 0\n",
    "        for segment in self.segments_list:\n",
    "            if segment['id'] not in inverse_dangling_pims:\n",
    "                while c_len in pims.keys():\n",
    "                    c_len+=1\n",
    "                pims[c_len] = {\"segment0\": [' '.join(text for text in segment['originalText']), segment['startTime'], segment['spokenBy'], segment['id']]}\n",
    "        return pims\n",
    "\n",
    "    def get_communities(self):\n",
    "        segments_data = ' '.join([sentence for segment in self.segments_list for sentence in segment['originalText']])\n",
    "        fv, graph_list = self.compute_feature_vector()\n",
    "        logger.info(\"No of sentences is\", extra={\"sentence\": len(fv.keys())})\n",
    "        meeting_graph, yetto_prune = self.construct_graph(fv, graph_list)\n",
    "        for v in [0.15, 0.1, 0.05, 0.01]:\n",
    "            flag = False\n",
    "            for i in range(5):\n",
    "                meeting_graph_pruned =  self.prune_edges(meeting_graph, graph_list, yetto_prune, v)\n",
    "                #community_set = community.best_partition(meeting_graph_pruned,randomize=False,random_state=9)\n",
    "                community_set = community.best_partition(meeting_graph_pruned)\n",
    "                mod = community.modularity(community_set, meeting_graph_pruned)\n",
    "                #logger.info(\"Meeting Graph results\", extra={\"edges before prunning\":meeting_graph.number_of_edges(), \"edges after prunning\": meeting_graph_pruned.number_of_edges()})\n",
    "                print ({\"edges before prunning\":meeting_graph.number_of_edges()}, {\"edges after prunning\": meeting_graph_pruned.number_of_edges()},{\"Modularity\": mod})\n",
    "                if mod>0.3:\n",
    "                    print (\"got modularity greater than 3. breaking.\")\n",
    "                    flag = True\n",
    "                    break\n",
    "                elif mod==0:\n",
    "                    meeting_graph_pruned = self.prune_edges(meeting_graph, graph_list, yetto_prune, 0.15)\n",
    "                    flag = True\n",
    "                    break\n",
    "            if flag:\n",
    "                break\n",
    "        #print({\"edges before prunning\":meeting_graph.number_of_edges()}, {\"edges after prunning\": meeting_graph_pruned.number_of_edges()})\n",
    "        community_set_sorted = self.compute_louvian_community(meeting_graph_pruned, community_set)\n",
    "        #print (community_set_sorted)\n",
    "        community_timerange = self.refine_community(community_set_sorted, graph_list)\n",
    "       #logger.info(\"commnity timerange\", extra={\"timerange\": community_timerange})\n",
    "        pims = self.group_community_by_time(community_timerange)\n",
    "        pims = self.wrap_community_by_time(pims)\n",
    "\n",
    "        logger.info(\"Final PIMs\", extra={\"PIMs\": pims})\n",
    "        return pims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T12:57:44.970154Z",
     "start_time": "2019-07-01T12:57:44.899564Z"
    }
   },
   "outputs": [],
   "source": [
    "def computepims(segments, model1):\n",
    "    community_extraction = community_detection(segments, model1)\n",
    "    pims = community_extraction.get_communities()\n",
    "    return pims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T12:57:45.233260Z",
     "start_time": "2019-07-01T12:57:45.033308Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"curl_test.json\", 'r') as f:\n",
    "    requests = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T12:58:01.072419Z",
     "start_time": "2019-07-01T12:57:45.266846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'v is : ': 0.15}\n",
      "{'edges before prunning': 11628} {'edges after prunning': 1746} {'Modularity': 0.20707698434254562}\n",
      "{'v is : ': 0.15}\n",
      "{'edges before prunning': 11628} {'edges after prunning': 1746} {'Modularity': 0.2053597553694978}\n",
      "{'v is : ': 0.15}\n",
      "{'edges before prunning': 11628} {'edges after prunning': 1746} {'Modularity': 0.20707698434254562}\n",
      "{'v is : ': 0.15}\n",
      "{'edges before prunning': 11628} {'edges after prunning': 1746} {'Modularity': 0.19635931056291}\n",
      "{'v is : ': 0.15}\n",
      "{'edges before prunning': 11628} {'edges after prunning': 1746} {'Modularity': 0.2053597553694978}\n",
      "{'v is : ': 0.1}\n",
      "{'edges before prunning': 11628} {'edges after prunning': 1164} {'Modularity': 0.23260936632774767}\n",
      "{'v is : ': 0.1}\n",
      "{'edges before prunning': 11628} {'edges after prunning': 1164} {'Modularity': 0.24647168491160942}\n",
      "{'v is : ': 0.1}\n",
      "{'edges before prunning': 11628} {'edges after prunning': 1164} {'Modularity': 0.22179377605366019}\n",
      "{'v is : ': 0.1}\n",
      "{'edges before prunning': 11628} {'edges after prunning': 1164} {'Modularity': 0.22962795668449823}\n",
      "{'v is : ': 0.1}\n",
      "{'edges before prunning': 11628} {'edges after prunning': 1164} {'Modularity': 0.2508867839302795}\n",
      "{'v is : ': 0.05}\n",
      "{'edges before prunning': 11628} {'edges after prunning': 583} {'Modularity': 0.30954370397394443}\n",
      "got modularity greater than 3. breaking.\n"
     ]
    }
   ],
   "source": [
    "segments = decode_json_request(requests)\n",
    "mind = selectmodel(\"01daayheky5f4e02qvrjptftxv\")\n",
    "model1 = loadmodel(bert_config, mind)\n",
    "#print (requests['segments'])\n",
    "res = computepims(segments, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T12:58:01.126216Z",
     "start_time": "2019-07-01T12:58:01.118812Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'segment0': [['All the changes accomplished complete on Subscription on two things I noticed test was that I IP address in the main link. So for that we I will discuss and and.'],\n",
       "   '2019-07-01T05:51:47Z',\n",
       "   '5aa6f27a-c06e-4569-9117-5797fafe8356',\n",
       "   '3a83f8cd4b014a62a3160e8cc318bff3'],\n",
       "  'segment1': [['Very and so that that will not happen and will that for the we need the then function that website video only allen.'],\n",
       "   '2019-07-01T05:52:05Z',\n",
       "   '5aa6f27a-c06e-4569-9117-5797fafe8356',\n",
       "   '56ae9ca08ba141fd83e39984660f9502'],\n",
       "  'segment2': [['Admin page right now it is just the basic so what the Colin had pointed out was that if we can make it look like the ether dashboard page in the background and and all that yeah.'],\n",
       "   '2019-07-01T05:52:39Z',\n",
       "   '5aa6f27a-c06e-4569-9117-5797fafe8356',\n",
       "   '9144799661c34afb9bd7ed9cfddd95cd'],\n",
       "  'segment3': [['Should I just remove that bton from website and in in the moment if the it is better.'],\n",
       "   '2019-07-01T05:54:35Z',\n",
       "   '5aa6f27a-c06e-4569-9117-5797fafe8356',\n",
       "   '322b1ac4fea0480cb2985e1f6be05cd3'],\n",
       "  'segment4': [['But I saying so at so that they know that it is it is if we can all ask for okay have app. Yeah, right right? Right? Yeah yeah. Okay? I will do that okay.'],\n",
       "   '2019-07-01T05:54:40Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '8862e2e928b14e34bf12f27140012386'],\n",
       "  'segment5': [['So that is one I think that is it right anyone else think of anything else before we just upload it and make it ready.'],\n",
       "   '2019-07-01T05:54:56Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   'ac19e25699b447b893d72735a6800b83'],\n",
       "  'segment6': [['What will do for is when make all the changes in that site you know website ether larger Io today and then we will ask Colin do take one quick pas and then if everything works. Well more go life. Okay. So one more thing is you do.'],\n",
       "   '2019-07-01T05:55:15Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '67e86760f99c442e9eb316c2d2af27d4'],\n",
       "  'segment7': [['Different transactions. I need to like it will be easier for to deploy the ether staging to the site staging two database on live on production. So all that changes which Colin on Friday could otherwise to manually do those changes on production of so I noticed that he had a dial account on the production. So it is fine. If we just remove that.'],\n",
       "   '2019-07-01T05:55:35Z',\n",
       "   '5aa6f27a-c06e-4569-9117-5797fafe8356',\n",
       "   '1ca17e5ef4f94971ab5038182af04e8a'],\n",
       "  'segment8': [['Yeah, sorry, the only thing to be is is that we do not in that and the myself the old spaces. I do not know if that will cause a problem or not I of okay.'],\n",
       "   '2019-07-01T05:56:46Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   'b441e316a05e4aa8a476b0937a308235'],\n",
       "  'segment9': [['Right and then on the slack page I our to kind of with info and update update that so that be fairly straightforward. We go wanted the stopper pending yes.'],\n",
       "   '2019-07-01T05:57:13Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '58580e14b22146be951ad0202a9d357b'],\n",
       "  'segment10': [['So I will work under like a log missing today and I work on auto beta sorry.'],\n",
       "   '2019-07-01T05:57:35Z',\n",
       "   'f591528c-d57a-46d6-9983-3dd9eb8d3868',\n",
       "   'ed01411790f64559b53202fee169261c'],\n",
       "  'segment11': [['I will Login like we have a.'],\n",
       "   '2019-07-01T05:57:50Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '9f07c0488e6a40ef8713756b6ad67e11'],\n",
       "  'segment12': [['Takes very like logic Wherein expires is after seven days. So like we need to change it to experience after seven days of log the user logs and the most will so right something like ready.'],\n",
       "   '2019-07-01T05:57:53Z',\n",
       "   'f591528c-d57a-46d6-9983-3dd9eb8d3868',\n",
       "   '4f77f58b16de4503aabd4c70e12dc84e'],\n",
       "  'segment13': [['Sorry that has been by one.'],\n",
       "   '2019-07-01T05:58:12Z',\n",
       "   '652ab717-de7e-4fee-b298-1f58c32b2774',\n",
       "   '50379020d13547649f8f484981d3e20b'],\n",
       "  'segment14': [['I work on auto data and then building it for windows.'],\n",
       "   '2019-07-01T05:58:21Z',\n",
       "   'f591528c-d57a-46d6-9983-3dd9eb8d3868',\n",
       "   'b01feac76b454e0b88db658563133f27'],\n",
       "  'segment15': [['From there is they see a new release correct correct yeah.'],\n",
       "   '2019-07-01T05:59:22Z',\n",
       "   'f591528c-d57a-46d6-9983-3dd9eb8d3868',\n",
       "   '75d6a94be2dd47d6a483b35e104f9132'],\n",
       "  'segment16': [['I for we to nine and before to hours to connect space. I am just plugging out the user that we r raised. Now it will be March today. Okay. Okay.'],\n",
       "   '2019-07-01T06:00:46Z',\n",
       "   '652ab717-de7e-4fee-b298-1f58c32b2774',\n",
       "   '85648324dd3b418c93ac95e0bdd29168'],\n",
       "  'segment17': [['Yeah I think a.'],\n",
       "   '2019-07-01T06:01:18Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   'ba0d93492bff4d68b8c55a3cf122ef38'],\n",
       "  'segment18': [['Okay Well check i mean this must be like a ownership issue right? Yeah so what will take is if.'],\n",
       "   '2019-07-01T06:02:09Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '6e755a3052dd4ae19ad77fc5a48d52ff'],\n",
       "  'segment19': [['In this were moving all the New recordings so.'],\n",
       "   '2019-07-01T06:02:55Z',\n",
       "   'a8734344-1592-4957-bd00-442188b39906',\n",
       "   'a74be0ca386148729595fa4f85616809'],\n",
       "  'segment20': [['I think this is really the last one last really critical one before we go right right? Yeah. Yes yes.'],\n",
       "   '2019-07-01T06:03:36Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '6eb544a6bdcc452f8b0f91ac2438d385'],\n",
       "  'segment21': [['Want we are done with this we dont need to do know create for each customer.'],\n",
       "   '2019-07-01T06:03:49Z',\n",
       "   'a8734344-1592-4957-bd00-442188b39906',\n",
       "   '2eef44ef1f3846f9ab1350432fdb8d1c'],\n",
       "  'segment22': [['On centralized how even the customer flow is also fine yeah.'],\n",
       "   '2019-07-01T06:03:55Z',\n",
       "   'a8734344-1592-4957-bd00-442188b39906',\n",
       "   '9f19af0accc34ed6a3a26c6a724e4017'],\n",
       "  'segment23': [['Looks like what happened it looks like I was let us say based know like Microsoft or.'],\n",
       "   '2019-07-01T06:05:02Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   'a92c39ba9089441ca1ecb0d415afaf03'],\n",
       "  'segment24': [['Denied so now it was but.'],\n",
       "   '2019-07-01T06:05:46Z',\n",
       "   '6f23149d-48aa-4f43-8529-4c7f9a40f88f',\n",
       "   'eed7b5860ee74e71b13b514034d04f95'],\n",
       "  'segment25': [['Check to properly is in the placing.'],\n",
       "   '2019-07-01T06:05:50Z',\n",
       "   '6f23149d-48aa-4f43-8529-4c7f9a40f88f',\n",
       "   '57f274c72c3443ba8a2fe85a58b26461'],\n",
       "  'segment26': [['Just to make sure that you know the right things are you know the right in the API well.'],\n",
       "   '2019-07-01T06:06:10Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '1fc49b8e5952443ba15946e93d06a7a7'],\n",
       "  'segment27': [['Anything else my.'],\n",
       "   '2019-07-01T06:06:20Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '6736da0a8129435cb529f373657c9355'],\n",
       "  'segment28': [['So there is my issue.'],\n",
       "   '2019-07-01T06:06:24Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '3bc1100b0b954a5eb81f0f3167cf5f44'],\n",
       "  'segment29': [['Launch issues because so there was a issue so I it a issue.'],\n",
       "   '2019-07-01T06:06:28Z',\n",
       "   '6f23149d-48aa-4f43-8529-4c7f9a40f88f',\n",
       "   '7353093e2b4d43e59eb7b85c6ebb1cfa'],\n",
       "  'segment30': [['One time taking one of add by.'],\n",
       "   '2019-07-01T06:06:38Z',\n",
       "   '6f23149d-48aa-4f43-8529-4c7f9a40f88f',\n",
       "   '7802f96b39334dacaa98c8ae22967d3b'],\n",
       "  'segment31': [['They might need someone to test.'],\n",
       "   '2019-07-01T06:06:48Z',\n",
       "   '6f23149d-48aa-4f43-8529-4c7f9a40f88f',\n",
       "   '12e54e16f88647e087608dc0a56fc18e'],\n",
       "  'segment32': [['So that I am thinking of the.'],\n",
       "   '2019-07-01T06:06:52Z',\n",
       "   '7d972d28-8644-4fcc-a360-f503c81abed6',\n",
       "   'fe03e8ea890141ee88eb8f492ead2350'],\n",
       "  'segment33': [['After the okay.'],\n",
       "   '2019-07-01T06:06:54Z',\n",
       "   '6f23149d-48aa-4f43-8529-4c7f9a40f88f',\n",
       "   'd311b5e49ea744718632c19009d8c8b6'],\n",
       "  'segment34': [['Yeah i.'],\n",
       "   '2019-07-01T06:07:08Z',\n",
       "   '7d972d28-8644-4fcc-a360-f503c81abed6',\n",
       "   '6079203236e24ea79c8e2016c4f85df4'],\n",
       "  'segment35': [['Might right? Yes you.'],\n",
       "   '2019-07-01T06:07:36Z',\n",
       "   '7d972d28-8644-4fcc-a360-f503c81abed6',\n",
       "   '69e20e16ad1942c5b9ce23526fdf08c6'],\n",
       "  'segment36': [['Some reason looking multiple just use.'],\n",
       "   '2019-07-01T06:07:40Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   'd24b1426dba1427ea81c135c56d04e99'],\n",
       "  'segment37': [['Joining actually some our package there is.'],\n",
       "   '2019-07-01T06:08:56Z',\n",
       "   '9465dff8-9d4f-4bf2-8b39-c76d3834f773',\n",
       "   '5cf276ac01c24c309cbbfad876823c5e'],\n",
       "  'segment38': [['I think we should be able to that by today and then have this going. So I think we have got our last couple of days we should resolve it today on the training. Okay such not god.'],\n",
       "   '2019-07-01T06:09:12Z',\n",
       "   '9465dff8-9d4f-4bf2-8b39-c76d3834f773',\n",
       "   '2769f07b2caa41b28c6527a29bebdc0b'],\n",
       "  'segment39': [['But.'],\n",
       "   '2019-07-01T06:10:00Z',\n",
       "   '9465dff8-9d4f-4bf2-8b39-c76d3834f773',\n",
       "   '0f905445ec9349128e71a5ef08b3d6ab'],\n",
       "  'segment40': [['We are trying to resolve yesterday on how to resolve that and then and then separate on. So the five and that will have.'],\n",
       "   '2019-07-01T06:10:50Z',\n",
       "   '9465dff8-9d4f-4bf2-8b39-c76d3834f773',\n",
       "   'ea07b2b98a2c4e9b95a49f96cdf73623'],\n",
       "  'segment41': [['Yeah they.'],\n",
       "   '2019-07-01T06:11:26Z',\n",
       "   '7d972d28-8644-4fcc-a360-f503c81abed6',\n",
       "   '498985a36ad0461a8fe0aca4240a7c43'],\n",
       "  'segment42': [['There are lot of lot of things that we think I there is a long call and if there is only one single speaker you have to take and also.'],\n",
       "   '2019-07-01T06:11:28Z',\n",
       "   '9465dff8-9d4f-4bf2-8b39-c76d3834f773',\n",
       "   '532178bcd18d45778bce0d38c349aa65'],\n",
       "  'segment43': [['Deep you all this in.'],\n",
       "   '2019-07-01T06:11:40Z',\n",
       "   '9465dff8-9d4f-4bf2-8b39-c76d3834f773',\n",
       "   'a8f203c0a66149adb076b9a27fdab80e'],\n",
       "  'segment44': [['So once.'],\n",
       "   '2019-07-01T06:11:48Z',\n",
       "   '7d972d28-8644-4fcc-a360-f503c81abed6',\n",
       "   'ea2e0944e86c4f948c3e6f3eec2d8200'],\n",
       "  'segment45': [['All three communities so that we should we should address that before we I start in the label with this other otherwise eleven to and differentiate the top is that direct all last week whether are we have this topic change that we manually good right updates on something it has that at that time has changed and topic which is really incorrect, but we just need to take it forward what to handle out the because perfect. Okay that is interesting and and then on the last one word I am meeting right? I am I am working on it wherein. We will have instead using both the to support that deep. So I have I have just I have that word we did not update that is the final but that still good but I am just back when good tool put that into staging do so that as well that we can do any enhancement or that that later.'],\n",
       "   '2019-07-01T06:12:02Z',\n",
       "   '9465dff8-9d4f-4bf2-8b39-c76d3834f773',\n",
       "   'a3c09e19033340d4ba8da71b3425c768'],\n",
       "  'segment46': [['Right? I evening anything else.'],\n",
       "   '2019-07-01T06:13:15Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '05d9de3a75b24a8480698c95af6cb2b3'],\n",
       "  'segment47': [['So I guess if you need to hang up at the same time right.'],\n",
       "   '2019-07-01T06:13:22Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   'b78f8bdf220b4929b260c1cc7cc9e70c'],\n",
       "  'segment48': [['Right? I evening anything else.'],\n",
       "   '2019-07-01T06:13:15Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '05d9de3a75b24a8480698c95af6cb2b3']},\n",
       " 1: {'segment0': ['It is.',\n",
       "   '2019-07-01T05:51:15Z',\n",
       "   '85070b77-d30a-438b-88a8-4ed4dfd27c93',\n",
       "   '5ad4dc5def7a457783ca80db4ae7183b']},\n",
       " 2: {'segment0': ['Okay.',\n",
       "   '2019-07-01T05:51:26Z',\n",
       "   '85070b77-d30a-438b-88a8-4ed4dfd27c93',\n",
       "   'd612f1710198449c873210743c00d7eb']},\n",
       " 3: {'segment0': ['We look like the so.',\n",
       "   '2019-07-01T05:52:25Z',\n",
       "   '5aa6f27a-c06e-4569-9117-5797fafe8356',\n",
       "   'f3ae05ce95e842a6a7e24a54bbecd1cf']},\n",
       " 4: {'segment0': ['Sorry, I missed the last one so that.',\n",
       "   '2019-07-01T05:52:36Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '47ca0405baa546c4a2371623d200c42c']},\n",
       " 5: {'segment0': ['After that mean.',\n",
       "   '2019-07-01T05:52:58Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   'fd008ea658d44272a8309c63eead7df4']},\n",
       " 6: {'segment0': ['Okay Yeah, we can get a basic side.',\n",
       "   '2019-07-01T05:53:01Z',\n",
       "   '5aa6f27a-c06e-4569-9117-5797fafe8356',\n",
       "   'd94ae4ae82844b25a284da523df60480']},\n",
       " 7: {'segment0': ['And then we be kind of whatever they there is something for.',\n",
       "   '2019-07-01T05:53:04Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '7cf8c69288014907b61d5020c73d5a3d']},\n",
       " 8: {'segment0': ['I like on higher party I can do that okay no I think.',\n",
       "   '2019-07-01T05:53:13Z',\n",
       "   '5aa6f27a-c06e-4569-9117-5797fafe8356',\n",
       "   '96c98a702e4841fa8665ebe75d4ce476']},\n",
       " 9: {'segment0': ['What I thinking about is if really like there is will be a couple of things right like one is getting a basic getting a website up. I think is good like we should stop just find a way to just update it we should remove the links to I download and Mac desktop top and windows download from now right the start windows. We can some over we can add it is done and we brought it with the view all that also on the emails we send we send today just send download you know what view that that email we send we say you start ether hope you can do with it and we also say go ahead and download the I and Desktop apps maybe at least I until those are approved and available maybe we should change that come sooner or something like that right okay so that is why you tell everything anything those are just need to be done for only this week until everything has come together yeah so should I add a coming soon.',\n",
       "   '2019-07-01T05:53:18Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '019412f4c46d4453b245d5ab7267bb89']},\n",
       " 10: {'segment0': ['Sorry which one sorry.',\n",
       "   '2019-07-01T05:56:08Z',\n",
       "   '5aa6f27a-c06e-4569-9117-5797fafe8356',\n",
       "   'bb61d924ab3c446caebfd19c29b36ab0']},\n",
       " 11: {'segment0': ['And as for few weeks like few changes which were basically stored on database. So it would be easier to directly like flush the data database of production and deploy that staging two database on production so that would lead to like you had created a trial account in dot dot. So that would just of that try now.',\n",
       "   '2019-07-01T05:56:13Z',\n",
       "   '5aa6f27a-c06e-4569-9117-5797fafe8356',\n",
       "   '9f5b38ed32e744728fbc5d2116108fff']},\n",
       " 12: {'segment0': ['Are those are okay only.',\n",
       "   '2019-07-01T05:56:42Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   'a6265203d12f4652aab58dd3341a7a57']},\n",
       " 13: {'segment0': ['Alright.',\n",
       "   '2019-07-01T05:57:08Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   'a78dbebb968946a2b0f03196fb16b382']},\n",
       " 14: {'segment0': ['Recently like weird yeah okay so like.',\n",
       "   '2019-07-01T05:58:16Z',\n",
       "   '5aa6f27a-c06e-4569-9117-5797fafe8356',\n",
       "   '7a4e910c88f144a9a99f15dc10a67d6d']},\n",
       " 15: {'segment0': ['I guess we cannot really pro until we got this upgrade problem right? Yeah yeah got got it. Okay yeah we to take care on that okay sounds good so yeah, just stay focused on that once that is done then we remove to this yeah and.',\n",
       "   '2019-07-01T05:59:28Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   'bef45ccc0e8a45eb82e13220e58abc8e']},\n",
       " 16: {'segment0': ['Right we talked about the hopefully this call will be able to test and everybody hands up when you see that that we have fixed for that or not.',\n",
       "   '2019-07-01T05:59:48Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '0a2f43d3aac94f30a1a1f537844eaea5']},\n",
       " 17: {'segment0': ['This one the recording phase, but this is let us check right very final in production like.',\n",
       "   '2019-07-01T06:00:03Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   'ba358a31fa5e4997a726a594f3ff4e40']},\n",
       " 18: {'segment0': ['Finally he had confirmed this was well like according page. This is test and the production okay so then you can go ahead.',\n",
       "   '2019-07-01T06:00:10Z',\n",
       "   '652ab717-de7e-4fee-b298-1f58c32b2774',\n",
       "   '9a9a905885e84fbf8ad9095e5974aa62']},\n",
       " 19: {'segment0': ['This one okay regarding the team.',\n",
       "   '2019-07-01T06:00:18Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '10ee5f8424c94c2e83c9a84d373f5612']},\n",
       " 20: {'segment0': ['Like branding name changes is done and in your of apps we they by the deep and only remaining parties to I have to make it sign. It is getting the left right now correct correct right let us be.',\n",
       "   '2019-07-01T06:00:24Z',\n",
       "   '652ab717-de7e-4fee-b298-1f58c32b2774',\n",
       "   '1887db2a08aa4d2db58c1b562206a4ea']},\n",
       " 21: {'segment0': ['Yeah okay I am regarding that.',\n",
       "   '2019-07-01T06:00:40Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '65b84854b8944e5b83ad189a27c61c80']},\n",
       " 22: {'segment0': ['He said the was actually license with the new line where and when once it got to the server backend. I just twenty days error. So if we directly improve it our one internal, but if we can go through the gateway there problem. So what will do we check know they it right and if you want we can know what do are thinking is in core the string to be sixty four and it and the server side I should be you know.',\n",
       "   '2019-07-01T06:01:21Z',\n",
       "   'a8734344-1592-4957-bd00-442188b39906',\n",
       "   '8bd8c322990a4932929530d230c47e2e']},\n",
       " 23: {'segment0': ['An an open issue enabling it or any solution code.',\n",
       "   '2019-07-01T06:02:17Z',\n",
       "   'a8734344-1592-4957-bd00-442188b39906',\n",
       "   '79d1018432e343d6b84e697dc7e7d82f']},\n",
       " 24: {'segment0': ['And other guys.',\n",
       "   '2019-07-01T06:02:23Z',\n",
       "   'a8734344-1592-4957-bd00-442188b39906',\n",
       "   '76d71a80725a46238773bc24c833105f']},\n",
       " 25: {'segment0': ['Customer markets yeah so.',\n",
       "   '2019-07-01T06:02:46Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   'd96f9341d51f4a48a02aff4f39b4c26d']},\n",
       " 26: {'segment0': ['You know essential, but the either i.',\n",
       "   '2019-07-01T06:02:59Z',\n",
       "   'a8734344-1592-4957-bd00-442188b39906',\n",
       "   '246000eeb77e42178d394490ca7250ff']},\n",
       " 27: {'segment0': ['Four and all so with so tested staging and let the play back.',\n",
       "   '2019-07-01T06:03:03Z',\n",
       "   'a8734344-1592-4957-bd00-442188b39906',\n",
       "   '23bd954c67b843da87a68422a1f229c3']},\n",
       " 28: {'segment0': ['Working fine so and the peer is also the.',\n",
       "   '2019-07-01T06:03:11Z',\n",
       "   'a8734344-1592-4957-bd00-442188b39906',\n",
       "   '6809d23e1eda4721b33bf59d363c3af0']},\n",
       " 29: {'segment0': ['That left as part of this you know.',\n",
       "   '2019-07-01T06:03:17Z',\n",
       "   'a8734344-1592-4957-bd00-442188b39906',\n",
       "   'e2d638b5ee64488486ff2984fc76401c']},\n",
       " 30: {'segment0': ['Get it done by today so that we can be deploy tomorrow and okay.',\n",
       "   '2019-07-01T06:03:29Z',\n",
       "   'a8734344-1592-4957-bd00-442188b39906',\n",
       "   '617ddcc37e9c4fbf8027612ae860179a']},\n",
       " 31: {'segment0': ['This is just a double shed.',\n",
       "   '2019-07-01T06:04:10Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '2821753c07394634bb1ad321a259aeee']},\n",
       " 32: {'segment0': ['Yesterday yesterday I think if you saw that do you and I have saw when a suddenly got us session expired.',\n",
       "   '2019-07-01T06:04:14Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '840e4c12aeec4f85807ff691342d920e']},\n",
       " 33: {'segment0': ['And also this.',\n",
       "   '2019-07-01T06:04:42Z',\n",
       "   '6f23149d-48aa-4f43-8529-4c7f9a40f88f',\n",
       "   '3dffde9c6666400db248f71c98d6c44c']},\n",
       " 34: {'segment0': ['Right? So in this case Factor example.',\n",
       "   '2019-07-01T06:04:59Z',\n",
       "   '7d972d28-8644-4fcc-a360-f503c81abed6',\n",
       "   'eecf7b3dd2bf4e098c7b08caefeb2422']},\n",
       " 35: {'segment0': ['To google the space in.',\n",
       "   '2019-07-01T06:05:14Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   'b8faf2218a65471f95670cfbb4b8f761']},\n",
       " 36: {'segment0': ['Tested these things.',\n",
       "   '2019-07-01T06:05:34Z',\n",
       "   '6f23149d-48aa-4f43-8529-4c7f9a40f88f',\n",
       "   '6e520a78f33b4fb0b5ae5bb086c31304']},\n",
       " 37: {'segment0': ['Yeah I did not see.',\n",
       "   '2019-07-01T06:06:04Z',\n",
       "   '7d972d28-8644-4fcc-a360-f503c81abed6',\n",
       "   'bd0b8be5f0314ad2bb62776fc2f6ef67']},\n",
       " 38: {'segment0': ['Okay So that is one.',\n",
       "   '2019-07-01T06:06:16Z',\n",
       "   '6f23149d-48aa-4f43-8529-4c7f9a40f88f',\n",
       "   '83c63944ebf64a5aae45ff4d22d2ccdd']},\n",
       " 39: {'segment0': ['One by one so there might.',\n",
       "   '2019-07-01T06:06:34Z',\n",
       "   '6f23149d-48aa-4f43-8529-4c7f9a40f88f',\n",
       "   'e32937dfa3be4a20abe84b862e95c41e']},\n",
       " 40: {'segment0': ['When after the Speaker changes.',\n",
       "   '2019-07-01T06:06:42Z',\n",
       "   '6f23149d-48aa-4f43-8529-4c7f9a40f88f',\n",
       "   'a2c7472802d942c88580a9e656765cb0']},\n",
       " 41: {'segment0': ['Involved.',\n",
       "   '2019-07-01T06:06:46Z',\n",
       "   '7d972d28-8644-4fcc-a360-f503c81abed6',\n",
       "   '1878c8b5703a400c8e6f6d1ff91261dc']},\n",
       " 42: {'segment0': ['Sounds good okay.',\n",
       "   '2019-07-01T06:06:58Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   'c5dd10d1da1f4faaa033faab2dc81249']},\n",
       " 43: {'segment0': ['A platform the I.',\n",
       "   '2019-07-01T06:07:03Z',\n",
       "   '7d972d28-8644-4fcc-a360-f503c81abed6',\n",
       "   '6b7f7b09a9164086b0748c02c092501e']},\n",
       " 44: {'segment0': ['I think you are extending.',\n",
       "   '2019-07-01T06:07:16Z',\n",
       "   '7d972d28-8644-4fcc-a360-f503c81abed6',\n",
       "   'b533704e70724a95a743a77ca2d232cb']},\n",
       " 45: {'segment0': ['He.',\n",
       "   '2019-07-01T06:07:19Z',\n",
       "   '9465dff8-9d4f-4bf2-8b39-c76d3834f773',\n",
       "   'eafcf95cea2044f6b1eefbc5da064643']},\n",
       " 46: {'segment0': ['There is a.',\n",
       "   '2019-07-01T06:07:25Z',\n",
       "   '9465dff8-9d4f-4bf2-8b39-c76d3834f773',\n",
       "   '01fd6e403f4343cda6dce19e1d0ef63d']},\n",
       " 47: {'segment0': ['Pending also have used okay.',\n",
       "   '2019-07-01T06:07:45Z',\n",
       "   '9465dff8-9d4f-4bf2-8b39-c76d3834f773',\n",
       "   '0f365c7ba6064420ac14896d07ba5e76']},\n",
       " 48: {'segment0': ['So can do it enough.',\n",
       "   '2019-07-01T06:07:57Z',\n",
       "   '7d972d28-8644-4fcc-a360-f503c81abed6',\n",
       "   '31889bb240b14b5195b9ad189c34c57f']},\n",
       " 49: {'segment0': ['That is that.',\n",
       "   '2019-07-01T06:08:20Z',\n",
       "   '7d972d28-8644-4fcc-a360-f503c81abed6',\n",
       "   'ebf01580c5a14ef79b7678ae0af25968']},\n",
       " 50: {'segment0': ['Do the multiple.',\n",
       "   '2019-07-01T06:08:38Z',\n",
       "   '9465dff8-9d4f-4bf2-8b39-c76d3834f773',\n",
       "   '1e5f496213604ab69f5c024bff08dee0']},\n",
       " 51: {'segment0': ['With switching theory should help it on.',\n",
       "   '2019-07-01T06:08:52Z',\n",
       "   '9465dff8-9d4f-4bf2-8b39-c76d3834f773',\n",
       "   '5fa0382d54d44f25b265bb57c8369a06']},\n",
       " 52: {'segment0': ['With the code for that so there is some in the ways package is that they are using.',\n",
       "   '2019-07-01T06:09:04Z',\n",
       "   '9465dff8-9d4f-4bf2-8b39-c76d3834f773',\n",
       "   'dbf76b7d73a84d4a922429c7d0a0e814']},\n",
       " 53: {'segment0': ['Instead of this that we do not know.',\n",
       "   '2019-07-01T06:09:50Z',\n",
       "   '9465dff8-9d4f-4bf2-8b39-c76d3834f773',\n",
       "   'be8d77456fbe4c099b378782fff31bcf']},\n",
       " 54: {'segment0': ['Okay.',\n",
       "   '2019-07-01T06:10:20Z',\n",
       "   '7d972d28-8644-4fcc-a360-f503c81abed6',\n",
       "   'fdc595b638aa4d78839a062bd5ee2a2b']},\n",
       " 55: {'segment0': ['We last time day we have this results on the for one time meeting we have you and India has not shared it that but we have the results after from sharing them.',\n",
       "   '2019-07-01T06:10:26Z',\n",
       "   '9465dff8-9d4f-4bf2-8b39-c76d3834f773',\n",
       "   '2e96f004d92f4ee7a63623ada137e4d1']},\n",
       " 56: {'segment0': ['Not.',\n",
       "   '2019-07-01T06:10:46Z',\n",
       "   '9465dff8-9d4f-4bf2-8b39-c76d3834f773',\n",
       "   '21e9f8d2d2224145991b84a37f2634ba']},\n",
       " 57: {'segment0': ['With call.',\n",
       "   '2019-07-01T06:11:12Z',\n",
       "   '9465dff8-9d4f-4bf2-8b39-c76d3834f773',\n",
       "   'a1a816e656e7476cb3a4ac157363a52c']},\n",
       " 58: {'segment0': ['That could one strategy to use it only for long calls.',\n",
       "   '2019-07-01T06:11:18Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   'de680bc4686e47978e3d148781b29acb']},\n",
       " 59: {'segment0': ['Yes for this I it is the ether podcast if if.',\n",
       "   '2019-07-01T06:11:52Z',\n",
       "   '9465dff8-9d4f-4bf2-8b39-c76d3834f773',\n",
       "   '534a1121e1414aa0a0ecd8d8fee24e34']},\n",
       " 60: {'segment0': ['Sounds good yeah.',\n",
       "   '2019-07-01T06:13:10Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '5b2be678874846d7a77db44572f0da7f']},\n",
       " 61: {'segment0': ['Alright, let us go on to zero.',\n",
       "   '2019-07-01T06:13:28Z',\n",
       "   '9dd355fc-fa0f-4a2d-ae05-756cca5a2a57',\n",
       "   '51fde58b032b478d8e52432e37be4be8']}}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "community",
   "language": "python",
   "name": "community"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

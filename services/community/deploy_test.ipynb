{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T17:51:40.126923Z",
     "start_time": "2019-07-04T17:51:37.820903Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ray__/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"/home/ray__/CS/org/etherlabs/hinton/community_detection/\")\n",
    "import json as js\n",
    "import text_preprocessing.preprocess as tp\n",
    "import torch\n",
    "import logging\n",
    "import networkx as nx\n",
    "import math\n",
    "from scipy.spatial.distance import cosine\n",
    "import community\n",
    "from datetime import datetime\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertModel\n",
    "from pytorch_pretrained_bert.modeling import BertPreTrainedModel, BertPreTrainingHeads\n",
    "import numpy as np\n",
    "import iso8601\n",
    "bert_config = {}\n",
    "bert_config[\"tokenizer\"] = 'bert-base-uncased'\n",
    "bert_config[\"config\"] = '/home/ray__/CS/org/etherlabs/ai-engine_pants/ai-engine/mind_files/bert_config.json'\n",
    "bert_config[\"bert_model\"] = 'bert-base-uncased'\n",
    "bert_config[\"load_file\"] = \"/home/ray__/CS/org/etherlabs/ai-engine_pants/ai-engine/mind_files/\"\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "class BertForPreTraining_custom(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForPreTraining_custom, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None):\n",
    "        output_all_encoded_layers=True\n",
    "        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n",
    "                                                   output_all_encoded_layers=output_all_encoded_layers)\n",
    "        if output_all_encoded_layers:\n",
    "            sequence_output_pred = sequence_output[-1]\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output_pred, pooled_output)\n",
    "\n",
    "        return prediction_scores, seq_relationship_score, sequence_output, pooled_output \n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def getNSPScore(sample_text):\n",
    "    m = torch.nn.Softmax()\n",
    "\n",
    "    tokenized_text = tokenizer.tokenize(sample_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = [0]*tokenized_text.index('[SEP]')+[1]*(len(tokenized_text)-tokenized_text.index('[SEP]'))\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    pred_score, seq_rel, seq_out, pool_out = model1(tokens_tensor, segments_tensors)\n",
    "    return m(seq_rel).detach().numpy()[0][0] #returns probability of being next sentence\n",
    "\n",
    "def getSentMatchScore(sent1, sent2, nsp_dampening_factor = 0.7):\n",
    "    sent1_feats = getBERTFeatures(model1, sent1, attn_head_idx)\n",
    "    sent2_feats = getBERTFeatures(model1, sent2, attn_head_idx)\n",
    "    cosine_distance = 1- cosine(sent1_feats, sent2_feats)\n",
    "    nsp_input1 = sent1+' [SEP] '+sent2\n",
    "    nsp_input2 = sent2+' [SEP] '+sent1\n",
    "    nsp_score_1 = getNSPScore(nsp_input1)\n",
    "    nsp_score_2 = getNSPScore(nsp_input2)\n",
    "    nsp_score = np.mean([nsp_score_1,nsp_score_2])*nsp_dampening_factor\n",
    "    len_diff = abs(len(sent1.split(' '))-len(sent2.split(' ')))\n",
    "    if len_diff>2*(min(len(sent1.split(' ')),len(sent2.split(' ')))):\n",
    "        #give more weight to nsp if the sentences of largely varying lengths\n",
    "        score = 0.4*cosine_distance+0.6*nsp_score\n",
    "    else:\n",
    "        score = np.mean([cosine_distance,nsp_score])\n",
    "    #print (\"nsp score -> \" + str(nsp_score))\n",
    "    #print (\"cosine score -> \" + str(cosine_distance))\n",
    "    return score\n",
    "\n",
    "def getSentMatchScore_wfeature(sent1, sent2, sent1_feats, sent2_feats, nsp_dampening_factor = 0.7):\n",
    "    cosine_distance = 1-cosine(sent1_feats, sent2_feats)\n",
    "    #return cosine_distance\n",
    "    nsp_input1 = sent1+' [SEP] '+sent2\n",
    "    #nsp_input2 = sent2+' [SEP] '+sent1\n",
    "    nsp_score_1 = getNSPScore(nsp_input1)\n",
    "    #nsp_score_2 = getNSPScore(nsp_input2)\n",
    "    nsp_score = nsp_score_1 * nsp_dampening_factor\n",
    "    #nsp_score = nsp_score_1*nsp_dampening_factor\n",
    "    len_diff = abs(len(sent1.split(' '))-len(sent2.split(' ')))\n",
    "    if len_diff>2*(min(len(sent1.split(' ')),len(sent2.split(' ')))):\n",
    "        #give more weight to nsp if the sentences of largely varying lengths\n",
    "        score = 0.4*cosine_distance+0.6*nsp_score\n",
    "    else:\n",
    "        score = np.mean([cosine_distance,nsp_score])\n",
    "    return score\n",
    "\n",
    "def getSentMatchScore_wfeature_cosine(sent1, sent2, sent1_feats, sent2_feats, nsp_dampening_factor = 0.7):\n",
    "    cosine_distance = 1-cosine(sent1_feats, sent2_feats)\n",
    "    return cosine_distance\n",
    "\n",
    "def getSentMatchScore_wfeature_test(sent1, sent2, sent1_feats, sent2_feats, nsp_dampening_factor = 0.7):\n",
    "    cosine_distance = 1-cosine(sent1_feats, sent2_feats)\n",
    "    nsp_input1 = sent1+' [SEP] '+sent2\n",
    "    nsp_input2 = sent2+' [SEP] '+sent1\n",
    "    nsp_score_1 = getNSPScore(nsp_input1)\n",
    "    nsp_score_2 = getNSPScore(nsp_input2)\n",
    "    nsp_score = np.mean([nsp_score_1,nsp_score_2])*nsp_dampening_factor\n",
    "    #nsp_score = nsp_score_1*nsp_dampening_factor\n",
    "    len_diff = abs(len(sent1.split(' '))-len(sent2.split(' ')))\n",
    "    if len_diff>2*(min(len(sent1.split(' ')),len(sent2.split(' ')))):\n",
    "        #give more weight to nsp if the sentences of largely varying lengths\n",
    "        score = 0.4*cosine_distance+0.6*nsp_score\n",
    "    else:\n",
    "        score = np.mean([cosine_distance,nsp_score])\n",
    "    return score, cosine_distance, nsp_score\n",
    "\n",
    "def getBERTFeatures(model, text, attn_head_idx = -1): #attn_head_idx - index o[]\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    if len(tokenized_text)>200:\n",
    "        tokenized_text = tokenized_text[0:200]\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    _, _, seq_out, pool_out = model(tokens_tensor)\n",
    "    seq_out = list(getPooledFeatures(seq_out[attn_head_idx]).T)\n",
    "    #pool_out = list(pool_out.detach().numpy().T)\n",
    "    return seq_out\n",
    "\n",
    "def getPooledFeatures(np_array):\n",
    "    np_array = np_array.reshape(np_array.shape[1],np_array.shape[2]).detach().numpy()\n",
    "    np_array_mp = np.mean(np_array, axis=0).reshape(1, -1)\n",
    "    return np_array_mp\n",
    "\n",
    "def replaceContractions(text):\n",
    "    #text = text.lower()\n",
    "    c_filt_text = ''\n",
    "    for word in text.split(' '):\n",
    "        if word in contractions:\n",
    "            c_filt_text = c_filt_text+' '+contractions[word]\n",
    "        else:\n",
    "            c_filt_text = c_filt_text+' '+word\n",
    "    return c_filt_text\n",
    "\n",
    "def cleanText(text):\n",
    "    text = text.replace('\\\\n','')\n",
    "    text = text.replace('\\\\','')\n",
    "    #text = text.replace('\\t', '')\n",
    "    #text = re.sub('\\[(.*?)\\]','',text) #removes [this one]\n",
    "    text = re.sub('(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?\\s',\n",
    "                ' __url__ ',text) #remove urls\n",
    "    #text = re.sub('\\'','',text)\n",
    "    #text = re.sub(r'\\d+', ' __number__ ', text) #replaces numbers\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.replace('\\t', '')\n",
    "    text = text.replace('\\n', '')\n",
    "    return text\n",
    "\n",
    "def formatTime(tz_time, datetime_object=False):\n",
    "    isoTime = iso8601.parse_date(tz_time)\n",
    "    ts = isoTime.timestamp()\n",
    "    ts = datetime.utcfromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n",
    "\n",
    "    if datetime_object:\n",
    "        ts = datetime.fromisoformat(ts)\n",
    "    return ts\n",
    "\n",
    "def preprocess_text(text):\n",
    "    mod_texts_unfiltered = tp.preprocess(text, stop_words=False, remove_punct=False)\n",
    "    mod_texts = []\n",
    "\n",
    "    for index, sent in enumerate(mod_texts_unfiltered):\n",
    "        if len(sent.split(' '))<=6:\n",
    "            continue\n",
    "        mod_texts.append(sent)\n",
    "    return mod_texts\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List\n",
    "import logging\n",
    "import text_preprocessing.preprocess as tp\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Request:\n",
    "    segments: list\n",
    "\n",
    "def decode_json_request(body) -> Request:\n",
    "    req = body\n",
    "\n",
    "#     if isinstance(body, str):\n",
    "#         req = json.load(body)\n",
    "\n",
    "    def decode_segments(seg):\n",
    "        #segments_text = [sent for sent in list(map(lambda x:tp.preprocess(x['originalText'], stop_words=False, remove_punct=False), seg['segments']))  if len(sent.split(' '))>=7]\n",
    "        segments_text = list(map(lambda x:preprocess_text(x['originalText']), seg['segments']))\n",
    "        segments_data = seg['segments']\n",
    "        for index, segment in  enumerate(segments_data):\n",
    "            segments_data[index]['originalText'] = segments_text[index]\n",
    "        return segments_data\n",
    "\n",
    "    segments = decode_segments(req)\n",
    "    return Request(segments)\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertModel\n",
    "from pytorch_pretrained_bert.modeling import BertPreTrainedModel, BertPreTrainingHeads\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def loadmodel(model_config, mind):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_config['tokenizer'])\n",
    "    config_file = BertConfig.from_json_file(model_config['config'])\n",
    "    #logger.debug(\"config file\", extra={\"contains\": model_config})\n",
    "    bert_model = model_config['bert_model']\n",
    "    load_file = model_config['load_file'] + mind\n",
    "# config_file = BertConfig.from_json_file('services/community/bert_config.json')\n",
    "    model1 = BertForPreTraining_custom(config_file)\n",
    "    state_dict_1 = torch.load(load_file, map_location='cpu')\n",
    "    model1.load_state_dict(state_dict_1)\n",
    "    return model1\n",
    "\n",
    "def selectmodel(MindId):\n",
    "    return \"mind-\"+str(MindId)+\".bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T18:22:55.169822Z",
     "start_time": "2019-07-04T18:22:55.130284Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "class community_detection():\n",
    "    segments_list = []\n",
    "    model1 = None\n",
    "    def __init__(self, Request, model1):\n",
    "        self.segments_list = Request.segments\n",
    "        self.model1 = model1\n",
    "    #def parse_meeting(self, segments):\n",
    "    #    segments_data = list(map(lambda x: tp.preprocess(x['originalText'], stop_words=False, remove_punct=False), segments['segments']))\n",
    "    #    self.segments_list = segments['segments']\n",
    "    #    for index,seg in enumerate(self.segments_list):\n",
    "    #        self.segments_list[index]['originalText'] = segments_data[index]\n",
    "\n",
    "    def compute_feature_vector(self):\n",
    "        #tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        #config = BertConfig.from_json_file('services/community/bert_config.json')\n",
    "        #bert_model = 'bert-base-uncased'\n",
    "        #model1 = bert.BertForPreTraining_custom(config)\n",
    "        #state_dict_1 = torch.load('services/community/bert_10epc_se_1e-6_sl40.bin')\n",
    "        #state_dict_1 = torch.load('services/community/bert_10epc_inc_se+etherdata_1e-6_sl40_bt64.bin')\n",
    "        #model1.load_state_dict(state_dict_1)\n",
    "        #model1.eval()\n",
    "        graph_list = {}\n",
    "        fv = {}\n",
    "        index = 0\n",
    "        for segment in self.segments_list:\n",
    "            for sent in segment['originalText']:\n",
    "                if sent!='':\n",
    "                    graph_list[index] = (sent, segment['startTime'], segment['spokenBy'], segment['id'])\n",
    "                    fv[index] = getBERTFeatures(self.model1, sent, attn_head_idx=-1)\n",
    "                    index+=1\n",
    "        return fv, graph_list\n",
    "\n",
    "    def construct_graph(self, fv, graph_list):\n",
    "        meeting_graph = nx.Graph()\n",
    "        yetto_prune = []\n",
    "        c_weight = 0\n",
    "        for indexa, nodea in enumerate(graph_list.values()):\n",
    "            for indexb, nodeb in enumerate(graph_list.values()):\n",
    "                if indexb>indexa:\n",
    "                    c_weight = cosine(fv[indexa], fv[indexb])\n",
    "                    meeting_graph.add_edge(indexa, indexb, weight=c_weight)\n",
    "                    yetto_prune.append((indexa, indexb, c_weight))\n",
    "        return meeting_graph, yetto_prune\n",
    "\n",
    "    def prune_edges(self, meeting_graph, graph_list, yetto_prune,v=0.01):\n",
    "        yetto_prune = sorted(yetto_prune, key=lambda kv : kv[2], reverse=True)\n",
    "        yetto_prune = yetto_prune[:math.ceil(len(yetto_prune)*v)+1]\n",
    "        #logger.info(\"pruning value\", extra={\"v is : \": v})\n",
    "        print ({\"v is : \": v})\n",
    "        meeting_graph_pruned = nx.Graph()\n",
    "        for indexa, indexb, c_score in yetto_prune:\n",
    "            meeting_graph_pruned.add_edge(indexa, indexb)\n",
    "        return meeting_graph_pruned\n",
    "\n",
    "    def compute_louvian_community(self, meeting_graph_pruned, community_set):\n",
    "        #community_set = community.best_partition(meeting_graph_pruned,randomize=False,random_state=9)\n",
    "        #modularity_score = community.modularity(community_set, meeting_graph_pruned)\n",
    "        #logger.info(\"Community results\", extra={\"modularity score\":modularity_score})\n",
    "        #print ({\"modularity score\":modularity_score})\n",
    "        community_set_sorted = sorted(community_set.items(), key = lambda kv: kv[1], reverse=False)\n",
    "\n",
    "        return community_set_sorted\n",
    "\n",
    "    def refine_community(self, community_set_sorted, graph_list):\n",
    "        clusters = []\n",
    "        temp = []\n",
    "        prev_com = 0\n",
    "        for index, (word,cluster) in enumerate(community_set_sorted):\n",
    "            if prev_com==cluster:\n",
    "                temp.append(word)\n",
    "                if index==len(community_set_sorted)-1:\n",
    "                    clusters.append(temp)\n",
    "            else:\n",
    "                clusters.append(temp)\n",
    "                temp = []\n",
    "                prev_com = cluster\n",
    "                temp.append(word)\n",
    "        timerange = []\n",
    "        temp = []\n",
    "        \n",
    "        for cluster in clusters:\n",
    "            temp= []\n",
    "            for sent in cluster:\n",
    "                #temp.append(graph_list[sent])\n",
    "                #logger.info(\"segment values\", extra={\"segment\":self.segments_list})\n",
    "                temp.append(graph_list[sent])\n",
    "            if len(temp)!=0:\n",
    "                #temp = list(set(temp))\n",
    "                temp = list(temp)\n",
    "                temp = sorted(temp,key=lambda kv: kv[1], reverse=False)\n",
    "                timerange.append(temp)\n",
    "#         for index, cluster in enumerate(clusters):\n",
    "#             temp= []\n",
    "#             for sent in cluster:\n",
    "#                 temp2 = [(sentence,start_time,user) for sentence,start_time,user in segment_contents.values() if mod_texts[sent] in sentence]\n",
    "#                 if len(temp2)!=0:\n",
    "#                     temp.append(temp2[0])\n",
    "#             if len(temp)!=0:\n",
    "#                 temp = sorted(temp,key=lambda kv: kv[1])\n",
    "#                 timerange.append(temp)\n",
    "\n",
    "        return timerange\n",
    "\n",
    "    def group_community_by_time(self, timerange):\n",
    "        timerange_detailed = []\n",
    "        temp = []\n",
    "        flag = False\n",
    "        pims = {}\n",
    "        index_pim = 0\n",
    "        index_segment = 0\n",
    "        for index,com in enumerate(timerange):\n",
    "            temp = []\n",
    "            flag = False\n",
    "            #print (\"-----community-----\", index)\n",
    "            for (index1,(sent1,time1,user1, id1)), (index2,(sent2,time2,user2, id2)) in zip(enumerate(com[0:]),enumerate(com[1:])):\n",
    "                if id1!=id2:\n",
    "                    if ((formatTime( time2, True)-formatTime(time1, True)).seconds<=120):\n",
    "                        if (not flag):\n",
    "                            pims[index_pim] = {'segment'+str(index_segment):[sent1,time1,user1, id1]}\n",
    "                            index_segment+=1\n",
    "                            temp.append((sent1,time1,user1, id1))\n",
    "                        pims[index_pim]['segment'+str(index_segment)] = [sent2,time2,user2, id2]\n",
    "                        index_segment+=1\n",
    "                        temp.append((sent2,time2,user2,id2))\n",
    "                        flag=True\n",
    "                    else:\n",
    "                        if flag==True:\n",
    "                            index_pim+=1\n",
    "                            index_segment=0\n",
    "                        flag=False\n",
    "            if flag==True:\n",
    "                index_pim+=1\n",
    "                index_segment=0\n",
    "            timerange_detailed.append(temp)\n",
    "        return pims\n",
    "\n",
    "    def wrap_community_by_time(self, pims):\n",
    "        yet_to_combine = []\n",
    "        need_to_remove = []\n",
    "        inverse_dangling_pims = []\n",
    "        for index1,i in enumerate(pims.keys()):\n",
    "            for index2,j in enumerate(pims.keys()):\n",
    "                if index1!=index2:\n",
    "                    if pims[i]['segment0'][1] >= pims[j]['segment0'][1] and pims[i]['segment0'][1] <= pims[j]['segment'+str(len(pims[j].values())-1)][1]:\n",
    "                        if (j,i) not in yet_to_combine and i not in need_to_remove and j not in need_to_remove:\n",
    "                            yet_to_combine.append((i,j))\n",
    "                            need_to_remove.append(i)\n",
    "        for i,j in yet_to_combine:\n",
    "            for k in pims[i]:\n",
    "                if pims[i][k] not in pims[j].values():\n",
    "                    pims[j]['segment'+str(len(pims[j].values())-1)] = pims[i][k]\n",
    "                    continue\n",
    "        for i in need_to_remove:\n",
    "            pims.pop(i)\n",
    "\n",
    "        for index, p in enumerate(pims.keys()):\n",
    "            for seg in pims[p].keys():\n",
    "                pims[p][seg][0] = [' '.join(text for text in segment['originalText']) for segment in self.segments_list if segment['id']==pims[p][seg][3]]\n",
    "                inverse_dangling_pims.append(pims[p][seg][3])\n",
    "\n",
    "        c_len = 0\n",
    "        for segment in self.segments_list:\n",
    "            if segment['id'] not in inverse_dangling_pims:\n",
    "                while c_len in pims.keys():\n",
    "                    c_len+=1\n",
    "                pims[c_len] = {\"segment0\": [' '.join(text for text in segment['originalText']), segment['startTime'], segment['spokenBy'], segment['id']]}\n",
    "        return pims\n",
    "\n",
    "    def get_communities(self):\n",
    "        segments_data = ' '.join([sentence for segment in self.segments_list for sentence in segment['originalText']])\n",
    "        fv, graph_list = self.compute_feature_vector()\n",
    "        logger.info(\"No of sentences is\", extra={\"sentence\": len(fv.keys())})\n",
    "        meeting_graph, yetto_prune = self.construct_graph(fv, graph_list)\n",
    "        max_meeting_grap_pruned = None\n",
    "        max_community_set = None\n",
    "        max_mod = 0\n",
    "        for v in [0.15, 0.1, 0.05, 0.01]:\n",
    "            flag = False\n",
    "            for i in range(5):\n",
    "                meeting_graph_pruned =  self.prune_edges(meeting_graph, graph_list, yetto_prune, v)\n",
    "                #community_set = community.best_partition(meeting_graph_pruned,randomize=False,random_state=9)\n",
    "                community_set = community.best_partition(meeting_graph_pruned)\n",
    "                mod = community.modularity(community_set, meeting_graph_pruned)\n",
    "                print({\"edges before prunning\":meeting_graph.number_of_edges()}, {\"edges after prunning\": meeting_graph_pruned.number_of_edges()},{\"modularity\":mod})\n",
    "                #logger.info(\"Meeting Graph results\", extra={\"edges before prunning\":meeting_graph.number_of_edges(), \"edges after prunning\": meeting_graph_pruned.number_of_edges()})\n",
    "#                 print ({\"edges before prunning\":meeting_graph.number_of_edges()}, {\"edges after prunning\": meeting_graph_pruned.number_of_edges()},{\"Modularity\": mod})\n",
    "#                 if mod>0.3:\n",
    "#                     print (\"got modularity greater than 3. breaking.\")\n",
    "#                     flag = True\n",
    "#                     break\n",
    "#                 elif mod==0:\n",
    "#                     meeting_graph_pruned = self.prune_edges(meeting_graph, graph_list, yetto_prune, 0.15)\n",
    "#                     flag = True\n",
    "#                     break\n",
    "                if mod>max_mod and mod<5:\n",
    "                    max_meeting_grap_pruned = meeting_graph_pruned\n",
    "                    max_community_set = community_set\n",
    "                    max_mod = mod\n",
    "                    flag = True\n",
    "#             if flag:\n",
    "#                 break\n",
    "        meeting_graph_pruned = max_meeting_grap_pruned\n",
    "        community_set = max_community_set\n",
    "        mod = max_mod\n",
    "        print({\"edges before prunning\":meeting_graph.number_of_edges()}, {\"edges after prunning\": meeting_graph_pruned.number_of_edges()},{\"modularity\":mod})\n",
    "        community_set_sorted = self.compute_louvian_community(meeting_graph_pruned, community_set)\n",
    "        #print (community_set_sorted)\n",
    "        community_timerange = self.refine_community(community_set_sorted, graph_list)\n",
    "       #logger.info(\"commnity timerange\", extra={\"timerange\": community_timerange})\n",
    "        pims = self.group_community_by_time(community_timerange)\n",
    "        pims = self.wrap_community_by_time(pims)\n",
    "\n",
    "        logger.info(\"Final PIMs\", extra={\"PIMs\": pims})\n",
    "        return pims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T18:22:55.464599Z",
     "start_time": "2019-07-04T18:22:55.457855Z"
    }
   },
   "outputs": [],
   "source": [
    "def computepims(segments, model1):\n",
    "    community_extraction = community_detection(segments, model1)\n",
    "    pims = community_extraction.get_communities()\n",
    "    return pims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T18:22:56.518606Z",
     "start_time": "2019-07-04T18:22:56.516030Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"curl_test.json\", 'r') as f:\n",
    "    requests = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T18:23:14.018707Z",
     "start_time": "2019-07-04T18:22:57.067303Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'v is : ': 0.15}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 1090} {'modularity': 0.17023356619813151}\n",
      "{'v is : ': 0.15}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 1090} {'modularity': 0.17613205959094358}\n",
      "{'v is : ': 0.15}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 1090} {'modularity': 0.182836882417305}\n",
      "{'v is : ': 0.15}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 1090} {'modularity': 0.16451350896389197}\n",
      "{'v is : ': 0.15}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 1090} {'modularity': 0.16891297028869626}\n",
      "{'v is : ': 0.1}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 727} {'modularity': 0.19650671959343757}\n",
      "{'v is : ': 0.1}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 727} {'modularity': 0.19079085537406654}\n",
      "{'v is : ': 0.1}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 727} {'modularity': 0.23456707200550964}\n",
      "{'v is : ': 0.1}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 727} {'modularity': 0.22830819122507948}\n",
      "{'v is : ': 0.1}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 727} {'modularity': 0.21542337317346824}\n",
      "{'v is : ': 0.05}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 364} {'modularity': 0.25125664170993844}\n",
      "{'v is : ': 0.05}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 364} {'modularity': 0.23856946624803765}\n",
      "{'v is : ': 0.05}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 364} {'modularity': 0.23173907136819227}\n",
      "{'v is : ': 0.05}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 364} {'modularity': 0.23029751841565027}\n",
      "{'v is : ': 0.05}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 364} {'modularity': 0.24264506098297306}\n",
      "{'v is : ': 0.01}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 74} {'modularity': 0.16736669101533969}\n",
      "{'v is : ': 0.01}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 74} {'modularity': 0.16736669101533969}\n",
      "{'v is : ': 0.01}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 74} {'modularity': 0.16736669101533969}\n",
      "{'v is : ': 0.01}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 74} {'modularity': 0.16736669101533969}\n",
      "{'v is : ': 0.01}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 74} {'modularity': 0.16736669101533969}\n",
      "{'edges before prunning': 7260} {'edges after prunning': 364} {'modularity': 0.25125664170993844}\n"
     ]
    }
   ],
   "source": [
    "segments = decode_json_request(requests)\n",
    "mind = selectmodel(\"01daaqy88qzb19jqz5prjfr76y\")\n",
    "model1 = loadmodel(bert_config, mind)\n",
    "#print (requests['segments'])\n",
    "res = computepims(segments, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T18:23:14.037679Z",
     "start_time": "2019-07-04T18:23:14.033567Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([3, 4, 6, 8, 0, 1, 2, 5, 7, 9, 10])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T18:25:19.764962Z",
     "start_time": "2019-07-04T18:25:19.744895Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'segment0': [['Most of the guys is complaining they do not know those kinds of and the do not have jobs anyway they have asets and for the average small on poor guy something could not school they do not btons. So that is why I we are talking about it a use people are article of my cherry question and he explains how the and putting pay taxes and built some numbers at on different order could not understand and because up our school that would reach you that three types of and they joined him and say wrong and it is changing and we see it over over again, Amazon not paying taxes or you say people when call say rubber from there and knocking architect well there is all the three sites to our point and I will head tail in the edge from side of the for my side, but this everybody can do the same thing for everyone other then will want the task only for the now tax loss whatever everybody can to have the like financial and the reason I am been financial education. Now that education got have to pay taxes few will not buy what I do mega a million dollars and pay text that takes and membership that time that I playing monopoly that is both four greenhouse. I was one by in detail or the Mcdonald is one I right about Mcdonald is right cross. So sell, but they are might rainbow sets sec pay text and outside very sixteen billion dollars how much sacks actually going.'],\n",
       "  '2019-07-04T12:36:44Z',\n",
       "  'd78b6120-3951-4b66-8e5d-5a8f30b9b2a9',\n",
       "  '8bdc10ab65514e76b9470dbc2d488251'],\n",
       " 'segment1': [['That is all ego anyone can do it everyone most ask the education share learn about money when was my father but watching that is real estate said wow not only if you make the money that is a big problem much money how much you because you and so we totally say this is a lot. I was is true every you can do it but most people just do not do it sometimes like said they do not have the education you need to have a plan if you are going to go through things well that is not risky. I mean me that is what is risk is having on Java paying taxes and saving money for present.'],\n",
       "  '2019-07-04T12:38:33Z',\n",
       "  'd78b6120-3951-4b66-8e5d-5a8f30b9b2a9',\n",
       "  '77534ae9d66d4c41b4c8ee2fe259527c'],\n",
       " 'segment2': [['How we do it in our core we all this calls from the slightly different. Here is their story like what is the story is this degree. Alright chris so let us give an call this person we want hear one three. Now depends on your time and your experience your location. So maybe do a little bit of research on that because I just cannot give you the best answer for now for you specifically okay. So now we are to talk about users interface design what one are the responsibilities of UI designer how do you learn it. It is a good career all those kind things we are have tough out right now we are user experience design us all about user experiencing the product and having a label interaction with it user interface design is visual cousin on speak it is all a look feel and the interaction and the presentation of the product itself. It is something might be saying does not that help me have a better user experience yes, it does that is why there is across from Gray, but Ui is more closely related to the visual communication arts like graphic design has been picking colors and or you making things really be beautiful and aesthetically pleasing if you look at for u UI the responsibilities that you will find there are more related to design to web design to maybe even branding sign and also time front and development because having a good understanding of code design. Okay nowadays all on there is such a thing as face designer for things like copies side of Airplane and you know all that kind of stuff but as closely is know used in under the stuff now like websites and products, but you is to receive.'],\n",
       "  '2019-07-04T12:40:03Z',\n",
       "  'd78b6120-3951-4b66-8e5d-5a8f30b9b2a9',\n",
       "  'd7dab43fc31e413c92413bf4d4182110'],\n",
       " 'segment3': [['All of the needs and the structure and everything that somebody like to use your experience designer has done and then take that and put that into in attractive aesthetically pleasing platform to be received by users. Let me make this even simpler a uX designer is going pas on skeleton and the UI designer is going to put on the scan closed and direct that knife people responsibility does use entire and the biggest sprint is the look and feel of the product the UI designer will do a good compettive alysis of what is going on in that same market what is like the standard what is a good look and field join a design research to make sure that the product makes sense like the designer is not a children application of making a look like a for walk, then designer just gonna to do all of the graph additional design for project. If they interface for like an application or website. Were talking btons and interactions with all that kind of visual stuff nowadays standard for you runners to also be thinking about animation interactions to the a UI designer also their design in every screen site our illusion. So whether there websites being viewed on a desktop for mobile device or maybe an cost three have to look good in all of those different aspects and then also the designer you is responsible for interacting was developer and making sure that things look and feel and all work the way that they should this report to development and the UX and the UI team all kind of come together and they making sure everything that is supposed to be every step way a UI designer. It is not necessarily a branding designer. Although they are closely tied to the brand of a company of product and they need to implement that brand through the design of the.'],\n",
       "  '2019-07-04T12:42:03Z',\n",
       "  'd78b6120-3951-4b66-8e5d-5a8f30b9b2a9',\n",
       "  '21f759cd73444630b2bfc3f12384842c'],\n",
       " 'segment4': [['I hope that helps to kind of differentiate and makes sense of the two and hopefully help you out if you are looking into the two fields figure out which ones right for you. I want does just stop and take a little and share a little bit about my personally.'],\n",
       "  '2019-07-04T12:46:03Z',\n",
       "  'd78b6120-3951-4b66-8e5d-5a8f30b9b2a9',\n",
       "  '4e47da2fc6c04eda9e1455568147ccf2']}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
